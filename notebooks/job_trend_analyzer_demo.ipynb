{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c6ee31",
   "metadata": {},
   "source": [
    "# üìä Job Trend Analyzer - Demo Notebook\n",
    "\n",
    "D·ª± √°n ph√¢n t√≠ch xu h∆∞·ªõng th·ªã tr∆∞·ªùng vi·ªác l√†m b·∫±ng c√°ch k·∫øt h·ª£p **n-gram + embedding + Gemini LLM Agent**, ƒë∆∞·ª£c tri·ªÉn khai chuy√™n nghi·ªáp theo ki·∫øn tr√∫c LangChain.\n",
    "\n",
    "## üéØ M·ª•c ti√™u\n",
    "- Ph√¢n t√≠ch job descriptions t·ª´ th·ªã tr∆∞·ªùng vi·ªác l√†m\n",
    "- Tr√≠ch xu·∫•t c√°c k·ªπ nƒÉng v√† c√¥ng ngh·ªá trending\n",
    "- Gom c·ª•m c√°c k·ªπ nƒÉng t∆∞∆°ng t·ª±\n",
    "- S·ª≠ d·ª•ng AI ƒë·ªÉ ph√¢n t√≠ch xu h∆∞·ªõng v√† ƒë∆∞a ra insights\n",
    "\n",
    "## üîÑ Lu·ªìng x·ª≠ l√Ω\n",
    "```\n",
    "Job Descriptions ‚Üí Text Cleaning ‚Üí N-gram Extraction ‚Üí Embedding ‚Üí Clustering ‚Üí LLM Analysis ‚Üí Trend Report\n",
    "```\n",
    "\n",
    "## ‚öôÔ∏è C√¥ng ngh·ªá s·ª≠ d·ª•ng\n",
    "- **Text Processing**: NLTK, scikit-learn\n",
    "- **Embeddings**: Together AI (m2-bert-80M-32k-retrieval)\n",
    "- **Clustering**: scikit-learn KMeans\n",
    "- **LLM Agent**: Google Gemini Pro\n",
    "- **Orchestrator**: LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb702575",
   "metadata": {},
   "source": [
    "## 1. üîß Setup Environment and Import Libraries\n",
    "\n",
    "ƒê·∫ßu ti√™n, ch√∫ng ta s·∫Ω c√†i ƒë·∫∑t v√† import t·∫•t c·∫£ c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if not installed)\n",
    "# !pip install together scikit-learn langchain google-generativeai sentence-transformers\n",
    "# !pip install pandas numpy nltk python-dotenv plotly\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project src to path\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.append('../src')\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "print(\"‚úÖ Environment setup completed!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly not available. Using matplotlib for visualization.\")\n",
    "\n",
    "# Import ML libraries\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    SKLEARN_AVAILABLE = True\n",
    "    print(\"‚úÖ Scikit-learn imported successfully\")\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"‚ùå Scikit-learn not available\")\n",
    "\n",
    "# Import NLP libraries\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"‚úÖ NLTK imported successfully\")\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"‚ùå NLTK not available\")\n",
    "\n",
    "# Import API clients\n",
    "try:\n",
    "    from together import Together\n",
    "    TOGETHER_AVAILABLE = True\n",
    "    print(\"‚úÖ Together AI client imported successfully\")\n",
    "except ImportError:\n",
    "    TOGETHER_AVAILABLE = False\n",
    "    print(\"‚ùå Together AI client not available\")\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_AVAILABLE = True\n",
    "    print(\"‚úÖ Google Gemini AI imported successfully\")\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    print(\"‚ùå Google Gemini AI not available\")\n",
    "\n",
    "try:\n",
    "    from langchain.tools import tool\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "    print(\"‚úÖ LangChain imported successfully\")\n",
    "except ImportError:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "    print(\"‚ùå LangChain not available\")\n",
    "\n",
    "print(\"\\nüìä Import Summary:\")\n",
    "print(f\"- Scikit-learn: {'‚úÖ' if SKLEARN_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"- NLTK: {'‚úÖ' if NLTK_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"- Together AI: {'‚úÖ' if TOGETHER_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"- Gemini AI: {'‚úÖ' if GEMINI_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"- LangChain: {'‚úÖ' if LANGCHAIN_AVAILABLE else '‚ùå'}\")\n",
    "print(f\"- Plotly: {'‚úÖ' if PLOTLY_AVAILABLE else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f865776",
   "metadata": {},
   "source": [
    "## 2. üîë Configure API Keys and Settings\n",
    "\n",
    "Thi·∫øt l·∫≠p c·∫•u h√¨nh cho c√°c API keys v√† parameters c·∫ßn thi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è python-dotenv not available. Set environment variables manually.\")\n",
    "\n",
    "# Configuration class\n",
    "class Config:\n",
    "    \"\"\"Configuration for the Job Trend Analyzer\"\"\"\n",
    "    \n",
    "    # API Keys (load from environment or set manually)\n",
    "    TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\", \"your_together_api_key_here\")\n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"your_gemini_api_key_here\")\n",
    "    \n",
    "    # Model configurations\n",
    "    EMBEDDING_MODEL = \"togethercomputer/m2-bert-80M-32k-retrieval\"\n",
    "    LLM_MODEL = \"gemini-pro\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    NGRAM_RANGE = (1, 3)  # Unigrams to trigrams\n",
    "    TOP_K_NGRAMS = 50\n",
    "    N_CLUSTERS = 8\n",
    "    MIN_WORD_LENGTH = 2\n",
    "    \n",
    "    # API parameters\n",
    "    MAX_RETRIES = 3\n",
    "    RETRY_DELAY = 1.0\n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Validate API keys\n",
    "print(\"üîç Checking API Configuration:\")\n",
    "print(f\"- Together API Key: {'‚úÖ Set' if config.TOGETHER_API_KEY != 'your_together_api_key_here' else '‚ùå Not set'}\")\n",
    "print(f\"- Gemini API Key: {'‚úÖ Set' if config.GEMINI_API_KEY != 'your_gemini_api_key_here' else '‚ùå Not set'}\")\n",
    "\n",
    "if config.TOGETHER_API_KEY == \"your_together_api_key_here\":\n",
    "    print(\"\\n‚ö†Ô∏è Please set your Together API key:\")\n",
    "    print(\"   1. Get API key from: https://api.together.xyz/\")\n",
    "    print(\"   2. Set environment variable: TOGETHER_API_KEY=your_key\")\n",
    "    print(\"   3. Or update the Config class above\")\n",
    "\n",
    "if config.GEMINI_API_KEY == \"your_gemini_api_key_here\":\n",
    "    print(\"\\n‚ö†Ô∏è Please set your Gemini API key:\")\n",
    "    print(\"   1. Get API key from: https://makersuite.google.com/app/apikey\")\n",
    "    print(\"   2. Set environment variable: GEMINI_API_KEY=your_key\")\n",
    "    print(\"   3. Or update the Config class above\")\n",
    "\n",
    "print(f\"\\nüìã Processing Configuration:\")\n",
    "print(f\"- Embedding Model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"- LLM Model: {config.LLM_MODEL}\")\n",
    "print(f\"- N-gram Range: {config.NGRAM_RANGE}\")\n",
    "print(f\"- Top K N-grams: {config.TOP_K_NGRAMS}\")\n",
    "print(f\"- Number of Clusters: {config.N_CLUSTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac2737",
   "metadata": {},
   "source": [
    "## 3. üìù Text Preprocessing Module\n",
    "\n",
    "X√¢y d·ª±ng module ƒë·ªÉ l√†m s·∫°ch v√† x·ª≠ l√Ω vƒÉn b·∫£n job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b51076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data if needed\n",
    "if NLTK_AVAILABLE:\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"üì• Downloading NLTK data...\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and preprocess job description text\n",
    "    \n",
    "    Args:\n",
    "        text: Raw job description text\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs and email addresses\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'[\\+]?[1-9]?[0-9]{7,15}', '', text)\n",
    "    \n",
    "    # Remove special characters but keep letters, numbers, and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove job-specific stopwords\n",
    "    job_stopwords = {\n",
    "        'job', 'position', 'role', 'candidate', 'applicant', 'experience',\n",
    "        'work', 'company', 'team', 'office', 'location', 'salary',\n",
    "        'benefit', 'requirement', 'qualification', 'responsibility',\n",
    "        'opportunity', 'career', 'employment', 'hire', 'hiring', 'year', 'years'\n",
    "    }\n",
    "    \n",
    "    # Get English stopwords\n",
    "    if NLTK_AVAILABLE:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(job_stopwords)\n",
    "        \n",
    "        # Tokenize and filter\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens \n",
    "                 if token.lower() not in stop_words \n",
    "                 and len(token) >= config.MIN_WORD_LENGTH \n",
    "                 and token.isalpha()]\n",
    "    else:\n",
    "        # Simple tokenization if NLTK not available\n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens \n",
    "                 if token.lower() not in job_stopwords \n",
    "                 and len(token) >= config.MIN_WORD_LENGTH \n",
    "                 and token.isalpha()]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the preprocessing function\n",
    "test_job_desc = \"\"\"\n",
    "We are looking for a Senior Python Developer with 5+ years of experience \n",
    "in machine learning and data science. The candidate should have expertise \n",
    "in TensorFlow, PyTorch, and scikit-learn. \n",
    "\n",
    "Requirements:\n",
    "- Bachelor's degree in Computer Science\n",
    "- Experience with AWS/GCP cloud platforms\n",
    "- Strong knowledge of SQL and NoSQL databases\n",
    "- Excellent communication skills\n",
    "\n",
    "Salary: $120,000 - $150,000 per year\n",
    "Location: San Francisco, CA\n",
    "Email: jobs@company.com\n",
    "Phone: (555) 123-4567\n",
    "Visit our website: https://company.com\n",
    "\"\"\"\n",
    "\n",
    "print(\"üß™ Testing Text Preprocessing:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Original text:\")\n",
    "print(test_job_desc[:200] + \"...\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Cleaned text:\")\n",
    "cleaned = clean_text(test_job_desc)\n",
    "print(cleaned)\n",
    "print(f\"\\nLength reduction: {len(test_job_desc)} ‚Üí {len(cleaned)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c6586",
   "metadata": {},
   "source": [
    "## 4. üî§ N-gram Extraction Implementation\n",
    "\n",
    "Tri·ªÉn khai tr√≠ch xu·∫•t n-gram t·ª´ vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97551cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(texts: List[str], ngram_range: Tuple[int, int] = (1, 3), \n",
    "               top_k: int = 50, use_tfidf: bool = True) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract n-grams from a list of texts using sklearn\n",
    "    \n",
    "    Args:\n",
    "        texts: List of preprocessed texts\n",
    "        ngram_range: Range of n-gram sizes (min_n, max_n)\n",
    "        top_k: Number of top n-grams to return\n",
    "        use_tfidf: Whether to use TF-IDF or simple count\n",
    "        \n",
    "    Returns:\n",
    "        List of (ngram, score) tuples sorted by score\n",
    "    \"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        print(\"‚ùå Scikit-learn not available. Cannot extract n-grams.\")\n",
    "        return []\n",
    "    \n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    # Filter out empty texts\n",
    "    texts = [text for text in texts if text.strip()]\n",
    "    \n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Choose vectorizer\n",
    "        if use_tfidf:\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                ngram_range=ngram_range,\n",
    "                max_features=top_k * 2,  # Get more features for better selection\n",
    "                min_df=2,  # Must appear in at least 2 documents\n",
    "                max_df=0.8,  # Must not appear in more than 80% of documents\n",
    "                lowercase=True,\n",
    "                token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b'  # Only alphabetic tokens\n",
    "            )\n",
    "        else:\n",
    "            vectorizer = CountVectorizer(\n",
    "                ngram_range=ngram_range,\n",
    "                max_features=top_k * 2,\n",
    "                min_df=2,\n",
    "                max_df=0.8,\n",
    "                lowercase=True,\n",
    "                token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b'\n",
    "            )\n",
    "        \n",
    "        # Fit and transform\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Calculate scores (sum across documents)\n",
    "        scores = X.sum(axis=0).A1\n",
    "        \n",
    "        # Create list of (ngram, score) tuples\n",
    "        ngram_scores = list(zip(feature_names, scores))\n",
    "        \n",
    "        # Sort by score (descending) and take top_k\n",
    "        ngram_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return ngram_scores[:top_k]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting n-grams: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test n-gram extraction with sample data\n",
    "sample_texts = [\n",
    "    \"python developer machine learning tensorflow pytorch\",\n",
    "    \"java backend spring boot microservices aws\",\n",
    "    \"javascript react frontend angular html css\",\n",
    "    \"data scientist python pandas numpy machine learning\",\n",
    "    \"devops engineer kubernetes docker aws cloud\",\n",
    "    \"fullstack developer python javascript react postgresql\",\n",
    "    \"backend engineer java spring boot rest api\",\n",
    "    \"ai engineer deep learning tensorflow python\",\n",
    "    \"cloud architect aws azure kubernetes microservices\",\n",
    "    \"data engineer python sql spark hadoop\"\n",
    "]\n",
    "\n",
    "# Clean the sample texts\n",
    "cleaned_texts = [clean_text(text) for text in sample_texts]\n",
    "\n",
    "print(\"üß™ Testing N-gram Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample texts: {len(sample_texts)}\")\n",
    "print(f\"Cleaned texts: {len(cleaned_texts)}\")\n",
    "\n",
    "# Extract n-grams\n",
    "ngrams = get_ngrams(cleaned_texts, \n",
    "                   ngram_range=config.NGRAM_RANGE, \n",
    "                   top_k=20, \n",
    "                   use_tfidf=True)\n",
    "\n",
    "print(f\"\\nüìä Top 20 N-grams (TF-IDF):\")\n",
    "for i, (ngram, score) in enumerate(ngrams):\n",
    "    print(f\"{i+1:2d}. {ngram:<25} (score: {score:.3f})\")\n",
    "\n",
    "# Compare with count-based extraction\n",
    "ngrams_count = get_ngrams(cleaned_texts, \n",
    "                         ngram_range=config.NGRAM_RANGE, \n",
    "                         top_k=10, \n",
    "                         use_tfidf=False)\n",
    "\n",
    "print(f\"\\nüìä Top 10 N-grams (Count):\")\n",
    "for i, (ngram, score) in enumerate(ngrams_count):\n",
    "    print(f\"{i+1:2d}. {ngram:<25} (count: {score:.0f})\")\n",
    "\n",
    "# Analyze n-gram lengths\n",
    "if ngrams:\n",
    "    unigrams = [(n, s) for n, s in ngrams if len(n.split()) == 1]\n",
    "    bigrams = [(n, s) for n, s in ngrams if len(n.split()) == 2] \n",
    "    trigrams = [(n, s) for n, s in ngrams if len(n.split()) == 3]\n",
    "    \n",
    "    print(f\"\\nüìà N-gram Distribution:\")\n",
    "    print(f\"- Unigrams: {len(unigrams)}\")\n",
    "    print(f\"- Bigrams: {len(bigrams)}\")\n",
    "    print(f\"- Trigrams: {len(trigrams)}\")\n",
    "    \n",
    "    if bigrams:\n",
    "        print(f\"\\nTop Bigrams: {', '.join([n for n, s in bigrams[:5]])}\")\n",
    "    if trigrams:\n",
    "        print(f\"Top Trigrams: {', '.join([n for n, s in trigrams[:3]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad0c4f",
   "metadata": {},
   "source": [
    "## 5. üß† Embedding Generation with Together API\n",
    "\n",
    "T·∫°o vector embeddings cho c√°c n-gram s·ª≠ d·ª•ng Together AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(phrases: List[str], api_key: str = None) -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of phrases using Together API\n",
    "    \n",
    "    Args:\n",
    "        phrases: List of phrases to embed\n",
    "        api_key: Together API key (uses config if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        List of (phrase, embedding) tuples\n",
    "    \"\"\"\n",
    "    if not TOGETHER_AVAILABLE:\n",
    "        print(\"‚ùå Together AI not available. Cannot generate embeddings.\")\n",
    "        return []\n",
    "    \n",
    "    api_key = api_key or config.TOGETHER_API_KEY\n",
    "    \n",
    "    if api_key == \"your_together_api_key_here\":\n",
    "        print(\"‚ùå Please set your Together API key first.\")\n",
    "        return []\n",
    "    \n",
    "    if not phrases:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Initialize Together client\n",
    "        client = Together(api_key=api_key)\n",
    "        \n",
    "        result = []\n",
    "        print(f\"üîÑ Creating embeddings for {len(phrases)} phrases...\")\n",
    "        \n",
    "        for i, phrase in enumerate(phrases):\n",
    "            try:\n",
    "                response = client.embeddings.create(\n",
    "                    model=config.EMBEDDING_MODEL,\n",
    "                    input=phrase\n",
    "                )\n",
    "                \n",
    "                if response.data and len(response.data) > 0:\n",
    "                    embedding = response.data[0].embedding\n",
    "                    result.append((phrase, embedding))\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 5 == 0:\n",
    "                    print(f\"  Progress: {i + 1}/{len(phrases)} embeddings created\")\n",
    "                \n",
    "                # Small delay to be respectful to API\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to embed phrase '{phrase}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Successfully created {len(result)}/{len(phrases)} embeddings\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "# Create sample embeddings (if API key is available)\n",
    "if config.TOGETHER_API_KEY != \"your_together_api_key_here\" and TOGETHER_AVAILABLE:\n",
    "    print(\"üß™ Testing Embedding Generation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use top 10 n-grams for testing\n",
    "    test_phrases = [ngram for ngram, score in ngrams[:10]]\n",
    "    print(f\"Test phrases: {test_phrases}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = get_embeddings(test_phrases)\n",
    "    \n",
    "    if embeddings:\n",
    "        print(f\"\\nüìä Embedding Results:\")\n",
    "        print(f\"- Number of embeddings: {len(embeddings)}\")\n",
    "        print(f\"- Embedding dimension: {len(embeddings[0][1]) if embeddings else 0}\")\n",
    "        \n",
    "        # Show first few embeddings (truncated)\n",
    "        for i, (phrase, embedding) in enumerate(embeddings[:3]):\n",
    "            embedding_preview = embedding[:5] + ['...'] if len(embedding) > 5 else embedding\n",
    "            print(f\"  {phrase}: {embedding_preview}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        if embeddings:\n",
    "            all_embeddings = [emb for _, emb in embeddings]\n",
    "            embedding_matrix = np.array(all_embeddings)\n",
    "            \n",
    "            print(f\"\\nüìà Embedding Statistics:\")\n",
    "            print(f\"- Mean magnitude: {np.mean(np.linalg.norm(embedding_matrix, axis=1)):.3f}\")\n",
    "            print(f\"- Std magnitude: {np.std(np.linalg.norm(embedding_matrix, axis=1)):.3f}\")\n",
    "            print(f\"- Dimension: {embedding_matrix.shape[1]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping embedding generation - API key not available\")\n",
    "    print(\"Setting up mock embeddings for demonstration...\")\n",
    "    \n",
    "    # Create mock embeddings for testing without API\n",
    "    np.random.seed(42)\n",
    "    test_phrases = [ngram for ngram, score in ngrams[:10]]\n",
    "    embeddings = []\n",
    "    \n",
    "    for phrase in test_phrases:\n",
    "        # Create mock embedding (768 dimensions like m2-bert)\n",
    "        mock_embedding = np.random.normal(0, 1, 768).tolist()\n",
    "        embeddings.append((phrase, mock_embedding))\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(embeddings)} mock embeddings for testing\")\n",
    "    print(f\"   Embedding dimension: {len(embeddings[0][1]) if embeddings else 0}\")\n",
    "\n",
    "# Store embeddings for next steps\n",
    "embedding_data = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed92e62",
   "metadata": {},
   "source": [
    "## 6. üìä Clustering Implementation\n",
    "\n",
    "Gom c·ª•m c√°c embeddings ƒë·ªÉ t√¨m ra c√°c nh√≥m k·ªπ nƒÉng t∆∞∆°ng t·ª±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc55192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings(embeddings: List[Tuple[str, List[float]]], \n",
    "                      n_clusters: int = 5) -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Cluster embeddings using KMeans algorithm\n",
    "    \n",
    "    Args:\n",
    "        embeddings: List of (phrase, embedding) tuples\n",
    "        n_clusters: Number of clusters to create\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping cluster IDs to lists of phrases\n",
    "    \"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        print(\"‚ùå Scikit-learn not available. Cannot perform clustering.\")\n",
    "        return {}\n",
    "    \n",
    "    if not embeddings or len(embeddings) < n_clusters:\n",
    "        print(f\"‚ö†Ô∏è Not enough embeddings ({len(embeddings)}) for {n_clusters} clusters\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Extract phrases and vectors\n",
    "        phrases = [phrase for phrase, _ in embeddings]\n",
    "        vectors = np.array([embedding for _, embedding in embeddings])\n",
    "        \n",
    "        print(f\"üîÑ Clustering {len(phrases)} phrases into {n_clusters} clusters...\")\n",
    "        \n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(vectors)\n",
    "        \n",
    "        # Organize results into clusters\n",
    "        clusters = {i: [] for i in range(n_clusters)}\n",
    "        for phrase, label in zip(phrases, labels):\n",
    "            clusters[label].append(phrase)\n",
    "        \n",
    "        # Calculate clustering quality metrics\n",
    "        silhouette_avg = silhouette_score(vectors, labels)\n",
    "        \n",
    "        print(f\"‚úÖ Clustering completed!\")\n",
    "        print(f\"   Silhouette Score: {silhouette_avg:.3f}\")\n",
    "        print(f\"   Inertia: {kmeans.inertia_:.3f}\")\n",
    "        \n",
    "        # Display cluster sizes\n",
    "        cluster_sizes = [len(clusters[i]) for i in range(n_clusters)]\n",
    "        print(f\"   Cluster sizes: {cluster_sizes}\")\n",
    "        \n",
    "        return clusters\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during clustering: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Perform clustering on the embeddings\n",
    "if embedding_data:\n",
    "    print(\"üß™ Testing Clustering:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Cluster the embeddings\n",
    "    clusters = cluster_embeddings(embedding_data, n_clusters=min(5, len(embedding_data)))\n",
    "    \n",
    "    if clusters:\n",
    "        print(f\"\\nüìä Clustering Results:\")\n",
    "        print(f\"Number of clusters: {len(clusters)}\")\n",
    "        \n",
    "        # Display each cluster\n",
    "        for cluster_id, phrases in clusters.items():\n",
    "            print(f\"\\nüè∑Ô∏è Cluster {cluster_id + 1} ({len(phrases)} items):\")\n",
    "            for phrase in phrases:\n",
    "                print(f\"   ‚Ä¢ {phrase}\")\n",
    "    \n",
    "    # Visualize clusters if possible\n",
    "    if SKLEARN_AVAILABLE and embedding_data and len(embedding_data) > 1:\n",
    "        print(f\"\\nüìà Creating 2D visualization...\")\n",
    "        \n",
    "        # Extract vectors for PCA\n",
    "        vectors = np.array([embedding for _, embedding in embedding_data])\n",
    "        phrases = [phrase for phrase, _ in embedding_data]\n",
    "        \n",
    "        # Reduce to 2D using PCA\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        vectors_2d = pca.fit_transform(vectors)\n",
    "        \n",
    "        # Get cluster labels\n",
    "        if clusters:\n",
    "            labels = []\n",
    "            for phrase in phrases:\n",
    "                for cluster_id, cluster_phrases in clusters.items():\n",
    "                    if phrase in cluster_phrases:\n",
    "                        labels.append(cluster_id)\n",
    "                        break\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Plot points with cluster colors\n",
    "            for cluster_id in range(len(clusters)):\n",
    "                cluster_points = vectors_2d[np.array(labels) == cluster_id]\n",
    "                if len(cluster_points) > 0:\n",
    "                    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "                              label=f'Cluster {cluster_id + 1}', alpha=0.7, s=100)\n",
    "            \n",
    "            # Add labels for points\n",
    "            for i, phrase in enumerate(phrases):\n",
    "                plt.annotate(phrase, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                           xytext=(5, 5), textcoords='offset points', \n",
    "                           fontsize=8, alpha=0.7)\n",
    "            \n",
    "            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "            plt.title('2D Visualization of Skill Clusters (PCA)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"‚úÖ 2D visualization created\")\n",
    "            print(f\"   Total variance explained: {sum(pca.explained_variance_ratio_):.1%}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No embeddings available for clustering\")\n",
    "    clusters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081badb1",
   "metadata": {},
   "source": [
    "## 7. ü§ñ LLM Agent Integration with Gemini\n",
    "\n",
    "S·ª≠ d·ª•ng Google Gemini ƒë·ªÉ ph√¢n t√≠ch c√°c c·ª•m v√† t·∫°o insights v·ªÅ xu h∆∞·ªõng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50da479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(clusters: Dict[int, List[str]], api_key: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Analyze clusters using Google Gemini to generate trend insights\n",
    "    \n",
    "    Args:\n",
    "        clusters: Dictionary mapping cluster IDs to lists of phrases\n",
    "        api_key: Gemini API key (uses config if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Analysis text from Gemini\n",
    "    \"\"\"\n",
    "    if not GEMINI_AVAILABLE:\n",
    "        print(\"‚ùå Google Gemini AI not available. Cannot perform analysis.\")\n",
    "        return \"Gemini AI not available for analysis.\"\n",
    "    \n",
    "    api_key = api_key or config.GEMINI_API_KEY\n",
    "    \n",
    "    if api_key == \"your_gemini_api_key_here\":\n",
    "        print(\"‚ùå Please set your Gemini API key first.\")\n",
    "        return \"Gemini API key not configured.\"\n",
    "    \n",
    "    if not clusters:\n",
    "        return \"No clusters available for analysis.\"\n",
    "    \n",
    "    try:\n",
    "        # Configure Gemini\n",
    "        genai.configure(api_key=api_key)\n",
    "        model = genai.GenerativeModel(config.LLM_MODEL)\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = \\\"\\\"\\\"B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m c√¥ng ngh·ªá. \n",
    "D∆∞·ªõi ƒë√¢y l√† c√°c nh√≥m k·ªπ nƒÉng/c√¥ng ngh·ªá ƒë√£ ƒë∆∞·ª£c gom c·ª•m t·ª´ job descriptions:\n",
    "\n",
    "\\\"\\\"\\\"\\n        \n",
    "        for cluster_id, phrases in clusters.items():\n",
    "            prompt += f\\\"\\\\nNh√≥m {cluster_id + 1}: {', '.join(phrases[:10])}\\\"\\n            if len(phrases) > 10:\\n                prompt += f\\\" (v√† {len(phrases) - 10} k·ªπ nƒÉng kh√°c)\\\"\\n        \\n        prompt += \\\"\\\"\\\"\\\\n\\\\nH√£y ph√¢n t√≠ch v√† ƒë∆∞a ra nh·ªØng nh·∫≠n ƒë·ªãnh s√¢u s·∫Øc v·ªÅ:\\\\n1. Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng c·ªßa t·ª´ng nh√≥m k·ªπ nƒÉng\\\\n2. Nh·ªØng c√¥ng ngh·ªá/k·ªπ nƒÉng hot nh·∫•t hi·ªán t·∫°i\\\\n3. Nh·ªØng k·ªπ nƒÉng ƒëang suy gi·∫£m (n·∫øu c√≥)\\\\n4. D·ª± ƒëo√°n xu h∆∞·ªõng trong 1-2 nƒÉm t·ªõi\\\\n5. L·ªùi khuy√™n cho ng∆∞·ªùi t√¨m vi·ªác trong ng√†nh IT\\\\n\\\\nH√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt v√† chuy√™n nghi·ªáp.\\\"\\\"\\\"\\n        \\n        print(\\\"üîÑ Analyzing clusters with Gemini AI...\\\")\\n        \\n        # Generate response\\n        response = model.generate_content(\\n            prompt,\\n            generation_config={\\n                'temperature': 0.3,\\n                'max_output_tokens': 1000,\\n                'top_p': 0.8,\\n                'top_k': 40\\n            }\\n        )\\n        \\n        if response.text:\\n            print(\\\"‚úÖ Analysis completed!\\\")\\n            return response.text\\n        else:\\n            return \\\"No analysis generated.\\\"\\n            \\n    except Exception as e:\\n        print(f\\\"‚ùå Error during analysis: {e}\\\")\\n        return f\\\"Error during analysis: {e}\\\"\\n\\n# Perform cluster analysis\\nif clusters and config.GEMINI_API_KEY != \\\"your_gemini_api_key_here\\\" and GEMINI_AVAILABLE:\\n    print(\\\"üß™ Testing LLM Analysis:\\\")\\n    print(\\\"=\\\" * 50)\\n    \\n    analysis_result = analyze_clusters(clusters)\\n    \\n    print(\\\"\\\\nü§ñ AI Analysis Results:\\\")\\n    print(\\\"=\\\" * 50)\\n    print(analysis_result)\\n    \\nelse:\\n    print(\\\"‚ö†Ô∏è Skipping LLM analysis - API key not available or no clusters\\\")\\n    print(\\\"Generating mock analysis for demonstration...\\\")\\n    \\n    analysis_result = \\\"\\\"\\\"üîç PH√ÇN T√çCH XU H∆Ø·ªöNG TH·ªä TR∆Ø·ªúNG VI·ªÜC L√ÄM IT (Mock Analysis)\\n\\nüìà XU H∆Ø·ªöNG TƒÇNG TR∆Ø·ªûNG:\\n‚Ä¢ Nh√≥m AI/ML: Python, machine learning, deep learning ƒëang c√≥ xu h∆∞·ªõng tƒÉng m·∫°nh\\n‚Ä¢ Nh√≥m Cloud: AWS, Docker, Kubernetes ti·∫øp t·ª•c l√† nh·ªØng k·ªπ nƒÉng hot\\n‚Ä¢ Nh√≥m Frontend: React, JavaScript v·∫´n duy tr√¨ s·ª©c n√≥ng\\n\\nüî• K·ª∏ NƒÇNG HOT NH·∫§T:\\n1. Python - Ng√¥n ng·ªØ ƒëa nƒÉng, ·ª©ng d·ª•ng r·ªông r√£i\\n2. Machine Learning - Xu h∆∞·ªõng AI ƒëang b√πng n·ªï\\n3. Cloud Technologies - Chuy·ªÉn ƒë·ªïi s·ªë ƒë·∫©y nhu c·∫ßu cloud\\n4. React/JavaScript - Frontend development v·∫´n r·∫•t c·∫ßn\\n\\nüìâ K·ª∏ NƒÇNG ƒêANG SUY GI·∫¢M:\\n‚Ä¢ C√°c c√¥ng ngh·ªá legacy nh∆∞ VB.NET, Flash\\n‚Ä¢ M·ªôt s·ªë framework c≈© ƒëang ƒë∆∞·ª£c thay th·∫ø\\n\\nüîÆ D·ª± ƒêO√ÅN 1-2 NƒÇM T·ªöI:\\n‚Ä¢ AI/ML s·∫Ω ti·∫øp t·ª•c tƒÉng tr∆∞·ªüng m·∫°nh\\n‚Ä¢ Cloud-native development s·∫Ω tr·ªü th√†nh chu·∫©n\\n‚Ä¢ Low-code/No-code platforms s·∫Ω ph√°t tri·ªÉn\\n‚Ä¢ DevOps v√† automation c√†ng quan tr·ªçng\\n\\nüí° L·ªúI KHUY√äN:\\n1. T·∫≠p trung h·ªçc Python v√† machine learning\\n2. N·∫Øm v·ªØng cloud platforms (AWS/Azure)\\n3. Ph√°t tri·ªÉn k·ªπ nƒÉng full-stack\\n4. Lu√¥n c·∫≠p nh·∫≠t c√¥ng ngh·ªá m·ªõi\\\"\\\"\\\"\\n    \\n    print(\\\"\\\\nü§ñ Mock Analysis Results:\\\")\\n    print(\\\"=\\\" * 50)\\n    print(analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05330432",
   "metadata": {},
   "source": [
    "## 8. ‚öôÔ∏è LangChain Tool Creation\n",
    "\n",
    "T√≠ch h·ª£p to√†n b·ªô pipeline v√†o m·ªôt LangChain tool ƒë·ªÉ s·ª≠ d·ª•ng d·ªÖ d√†ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce54c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGCHAIN_AVAILABLE:\\n    @tool\\n    def analyze_job_trend(texts: List[str]) -> str:\\n        \\\"\\\"\\\"Analyze job market trends from job descriptions.\\n        \\n        Args:\\n            texts: List of job description texts\\n            \\n        Returns:\\n            Comprehensive trend analysis report\\n        \\\"\\\"\\\"\\n        try:\\n            print(\\\"üöÄ Starting Job Trend Analysis Pipeline...\\\")\\n            \\n            # Step 1: Clean texts\\n            print(\\\"1Ô∏è‚É£ Cleaning texts...\\\")\\n            cleaned = [clean_text(t) for t in texts if t.strip()]\\n            \\n            if not cleaned:\\n                return \\\"No valid texts provided for analysis.\\\"\\n            \\n            print(f\\\"   Processed {len(cleaned)} texts\\\")\\n            \\n            # Step 2: Extract n-grams\\n            print(\\\"2Ô∏è‚É£ Extracting n-grams...\\\")\\n            ngrams = get_ngrams(cleaned, \\n                               ngram_range=config.NGRAM_RANGE, \\n                               top_k=config.TOP_K_NGRAMS)\\n            \\n            if not ngrams:\\n                return \\\"No n-grams could be extracted from the texts.\\\"\\n                \\n            print(f\\\"   Extracted {len(ngrams)} n-grams\\\")\\n            \\n            # Step 3: Create embeddings\\n            print(\\\"3Ô∏è‚É£ Creating embeddings...\\\")\\n            phrases = [g[0] for g in ngrams[:20]]  # Limit for demo\\n            embeddings = get_embeddings(phrases)\\n            \\n            if not embeddings:\\n                return \\\"No embeddings could be created. Check API configuration.\\\"\\n                \\n            print(f\\\"   Created {len(embeddings)} embeddings\\\")\\n            \\n            # Step 4: Cluster embeddings\\n            print(\\\"4Ô∏è‚É£ Clustering embeddings...\\\")\\n            clusters = cluster_embeddings(embeddings, \\n                                        n_clusters=min(config.N_CLUSTERS, len(embeddings)))\\n            \\n            if not clusters:\\n                return \\\"Clustering failed. Not enough data.\\\"\\n                \\n            print(f\\\"   Created {len(clusters)} clusters\\\")\\n            \\n            # Step 5: Analyze with LLM\\n            print(\\\"5Ô∏è‚É£ Analyzing with AI...\\\")\\n            analysis = analyze_clusters(clusters)\\n            \\n            # Compile final report\\n            report = f\\\"\\\"\\\"# üìä JOB TREND ANALYSIS REPORT\\n\\n## üìà Data Summary\\n- Job descriptions analyzed: {len(texts)}\\n- Valid texts processed: {len(cleaned)}\\n- N-grams extracted: {len(ngrams)}\\n- Skill clusters identified: {len(clusters)}\\n\\n## üîù Top Trending Skills\\n\\\"\\\"\\\"\\n            \\n            for i, (ngram, score) in enumerate(ngrams[:10]):\\n                report += f\\\"{i+1}. {ngram} (score: {score:.2f})\\\\n\\\"\\n            \\n            report += f\\\"\\\\n## üè∑Ô∏è Skill Clusters\\\\n\\\"\\n            for cluster_id, phrases in clusters.items():\\n                report += f\\\"\\\\n**Cluster {cluster_id + 1}:** {', '.join(phrases[:5])}\\\"\\n                if len(phrases) > 5:\\n                    report += f\\\" (v√† {len(phrases) - 5} k·ªπ nƒÉng kh√°c)\\\"\\n                report += \\\"\\\\n\\\"\\n            \\n            report += f\\\"\\\\n## ü§ñ AI Analysis\\\\n{analysis}\\\"\\n            \\n            print(\\\"‚úÖ Pipeline completed successfully!\\\")\\n            return report\\n            \\n        except Exception as e:\\n            error_msg = f\\\"‚ùå Pipeline failed: {str(e)}\\\"\\n            print(error_msg)\\n            return error_msg\\n    \\n    print(\\\"‚úÖ LangChain tool 'analyze_job_trend' created successfully!\\\")\\n    print(\\\"Usage: result = analyze_job_trend.invoke({'texts': your_job_descriptions})\\\")\\n    \\nelse:\\n    print(\\\"‚ö†Ô∏è LangChain not available. Tool creation skipped.\\\")\\n    \\n    # Create a simple wrapper function instead\\n    def analyze_job_trend_simple(texts: List[str]) -> str:\\n        \\\"\\\"\\\"Simple version of the job trend analyzer without LangChain\\\"\\\"\\\"\\n        try:\\n            # Run the pipeline steps\\n            cleaned = [clean_text(t) for t in texts if t.strip()]\\n            ngrams = get_ngrams(cleaned, ngram_range=config.NGRAM_RANGE, top_k=20)\\n            phrases = [g[0] for g in ngrams[:10]]\\n            embeddings = get_embeddings(phrases) if phrases else []\\n            clusters = cluster_embeddings(embeddings, n_clusters=min(5, len(embeddings))) if embeddings else {}\\n            analysis = analyze_clusters(clusters) if clusters else \\\"No clusters available for analysis.\\\"\\n            \\n            # Create simple report\\n            report = f\\\"\\\"\\\"JOB TREND ANALYSIS\\\\n\\\\nProcessed {len(cleaned)} texts\\\\nTop skills: {', '.join([n for n, s in ngrams[:5]])}\\\\n\\\\nAnalysis:\\\\n{analysis}\\\"\\\"\\\"\\n            \\n            return report\\n            \\n        except Exception as e:\\n            return f\\\"Analysis failed: {str(e)}\\\"\\n    \\n    print(\\\"‚úÖ Simple analyzer function created as fallback\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1978f7",
   "metadata": {},
   "source": [
    "## 9. üîß Pipeline Integration Testing\n",
    "\n",
    "Ki·ªÉm tra ho·∫°t ƒë·ªông c·ªßa to√†n b·ªô pipeline v·ªõi d·ªØ li·ªáu m·∫´u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c918f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the integrated pipeline with small dataset\\ntest_job_descriptions = [\\n    \\\"Senior Python Developer with machine learning experience using TensorFlow and PyTorch\\\",\\n    \\\"Java Backend Engineer working with Spring Boot and microservices architecture\\\", \\n    \\\"Frontend Developer specializing in React, Angular, and modern JavaScript frameworks\\\",\\n    \\\"Data Scientist proficient in Python, pandas, scikit-learn, and statistical analysis\\\",\\n    \\\"DevOps Engineer experienced with AWS, Kubernetes, Docker, and CI/CD pipelines\\\"\\n]\\n\\nprint(\\\"üß™ Testing Complete Pipeline Integration:\\\")\\nprint(\\\"=\\\" * 60)\\nprint(f\\\"Test dataset: {len(test_job_descriptions)} job descriptions\\\")\\n\\n# Test individual components first\\nprint(\\\"\\\\nüîç Component Testing:\\\")\\nprint(\\\"-\\\" * 30)\\n\\n# Test preprocessing\\ntest_cleaned = [clean_text(desc) for desc in test_job_descriptions]\\nprint(f\\\"‚úÖ Preprocessing: {len(test_cleaned)} texts cleaned\\\")\\n\\n# Test n-gram extraction\\ntest_ngrams = get_ngrams(test_cleaned, ngram_range=(1, 2), top_k=15)\\nprint(f\\\"‚úÖ N-gram extraction: {len(test_ngrams)} n-grams extracted\\\")\\n\\nif test_ngrams:\\n    print(\\\"   Top 5 n-grams:\\\", [n for n, s in test_ngrams[:5]])\\n\\n# Test component integration\\nprint(\\\"\\\\nüîó Integration Testing:\\\")\\nprint(\\\"-\\\" * 30)\\n\\nstart_time = time.time()\\n\\ntry:\\n    if LANGCHAIN_AVAILABLE and 'analyze_job_trend' in locals():\\n        print(\\\"Testing LangChain tool...\\\")\\n        result = analyze_job_trend.invoke({\\\"texts\\\": test_job_descriptions})\\n    else:\\n        print(\\\"Testing simple analyzer...\\\")\\n        result = analyze_job_trend_simple(test_job_descriptions)\\n    \\n    execution_time = time.time() - start_time\\n    \\n    print(f\\\"\\\\n‚è±Ô∏è Execution time: {execution_time:.2f} seconds\\\")\\n    print(f\\\"\\\\nüìã Pipeline Result:\\\")\\n    print(\\\"=\\\" * 60)\\n    print(result)\\n    \\nexcept Exception as e:\\n    print(f\\\"‚ùå Pipeline test failed: {e}\\\")\\n    print(\\\"This is expected if API keys are not configured\\\")\\n\\n# Performance summary\\nprint(\\\"\\\\nüìä Performance Summary:\\\")\\nprint(\\\"=\\\" * 30)\\nprint(f\\\"- Input texts: {len(test_job_descriptions)}\\\")\\nprint(f\\\"- Processing time: {execution_time:.2f}s\\\" if 'execution_time' in locals() else \\\"- Processing time: N/A\\\")\\nprint(f\\\"- Average time per text: {(execution_time/len(test_job_descriptions)):.2f}s\\\" if 'execution_time' in locals() else \\\"- Average time per text: N/A\\\")\\n\\n# Component availability summary\\nprint(\\\"\\\\nüîß Component Status:\\\")\\nprint(\\\"=\\\" * 30)\\ncomponents = {\\n    \\\"Text Preprocessing\\\": True,\\n    \\\"N-gram Extraction\\\": SKLEARN_AVAILABLE,\\n    \\\"Embedding Generation\\\": TOGETHER_AVAILABLE and config.TOGETHER_API_KEY != \\\"your_together_api_key_here\\\",\\n    \\\"Clustering\\\": SKLEARN_AVAILABLE,\\n    \\\"LLM Analysis\\\": GEMINI_AVAILABLE and config.GEMINI_API_KEY != \\\"your_gemini_api_key_here\\\",\\n    \\\"LangChain Integration\\\": LANGCHAIN_AVAILABLE\\n}\\n\\nfor component, status in components.items():\\n    status_icon = \\\"‚úÖ\\\" if status else \\\"‚ùå\\\"\\n    print(f\\\"{status_icon} {component}\\\")\\n\\nfunctional_components = sum(components.values())\\ntotal_components = len(components)\\nprint(f\\\"\\\\nüìà Overall Status: {functional_components}/{total_components} components functional ({functional_components/total_components*100:.0f}%)\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca34a8b",
   "metadata": {},
   "source": [
    "## 10. üìà Sample Data Analysis\n",
    "\n",
    "Ch·∫°y ph√¢n t√≠ch tr√™n b·ªô d·ªØ li·ªáu l·ªõn h∆°n ƒë·ªÉ th·ªÉ hi·ªán kh·∫£ nƒÉng th·ª±c t·∫ø c·ªßa h·ªá th·ªëng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended sample dataset for comprehensive analysis\\nsample_job_dataset = [\\n    \\\"Senior Python Developer with 5+ years experience in machine learning, TensorFlow, PyTorch, and scikit-learn. Knowledge of AWS cloud services required.\\\",\\n    \\\"Java Backend Engineer to build scalable microservices using Spring Boot, REST APIs, and MySQL. Docker and Kubernetes experience preferred.\\\",\\n    \\\"Frontend React Developer proficient in JavaScript, TypeScript, Redux, and modern CSS frameworks. Experience with Next.js is a plus.\\\",\\n    \\\"Data Scientist with expertise in Python, pandas, NumPy, statistical analysis, and machine learning algorithms. SQL and data visualization skills required.\\\",\\n    \\\"DevOps Engineer experienced with AWS/Azure, Kubernetes, Docker, CI/CD pipelines, Infrastructure as Code, and monitoring tools.\\\",\\n    \\\"Full Stack Developer using MEAN/MERN stack. Proficiency in Node.js, Express, MongoDB, and Angular/React frameworks required.\\\",\\n    \\\"AI/ML Engineer to develop deep learning models using TensorFlow, PyTorch, computer vision, and natural language processing techniques.\\\",\\n    \\\"Cloud Architect to design scalable solutions on AWS/Azure platforms. Experience with serverless computing and microservices architecture.\\\",\\n    \\\"Mobile Developer creating cross-platform apps with React Native and Flutter. Knowledge of native iOS and Android development preferred.\\\",\\n    \\\"Cybersecurity Analyst with expertise in penetration testing, vulnerability assessment, network security, and incident response procedures.\\\",\\n    \\\"QA Engineer skilled in test automation, Selenium, API testing, and performance testing. Experience with Agile methodologies required.\\\",\\n    \\\"Product Manager with technical background in software development. Experience with user research, data analysis, and product strategy.\\\",\\n    \\\"Database Administrator proficient in MySQL, PostgreSQL, MongoDB, performance optimization, and backup/recovery procedures.\\\",\\n    \\\"UI/UX Designer with strong skills in Figma, Adobe Creative Suite, user research, prototyping, and responsive web design principles.\\\",\\n    \\\"Blockchain Developer experienced with Ethereum, Solidity, smart contracts, DeFi protocols, and Web3 technologies.\\\"\\n]\\n\\nprint(\\\"üöÄ COMPREHENSIVE JOB TREND ANALYSIS\\\")\\nprint(\\\"=\\\" * 60)\\nprint(f\\\"Analyzing {len(sample_job_dataset)} diverse job descriptions...\\\")\\nprint(\\\"This represents a realistic job market sample.\\\")\\n\\n# Run comprehensive analysis\\nstart_time = time.time()\\n\\ntry:\\n    # Choose the appropriate analyzer\\n    if LANGCHAIN_AVAILABLE and 'analyze_job_trend' in locals():\\n        print(\\\"\\\\nüîß Using LangChain-powered analyzer...\\\")\\n        final_result = analyze_job_trend.invoke({\\\"texts\\\": sample_job_dataset})\\n    else:\\n        print(\\\"\\\\nüîß Using simple analyzer...\\\")\\n        final_result = analyze_job_trend_simple(sample_job_dataset)\\n    \\n    total_time = time.time() - start_time\\n    \\n    print(f\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n    print(\\\"üìä FINAL ANALYSIS RESULTS\\\")\\n    print(\\\"=\\\" * 80)\\n    print(final_result)\\n    \\n    print(f\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n    print(\\\"‚è±Ô∏è PERFORMANCE METRICS\\\")\\n    print(\\\"=\\\" * 80)\\n    print(f\\\"‚Ä¢ Total processing time: {total_time:.2f} seconds\\\")\\n    print(f\\\"‚Ä¢ Average time per job description: {total_time/len(sample_job_dataset):.2f} seconds\\\")\\n    print(f\\\"‚Ä¢ Dataset size: {len(sample_job_dataset)} job descriptions\\\")\\n    print(f\\\"‚Ä¢ Text processing rate: {len(sample_job_dataset)/total_time:.1f} jobs/second\\\")\\n    \\nexcept Exception as e:\\n    print(f\\\"\\\\n‚ùå Comprehensive analysis failed: {e}\\\")\\n    print(\\\"\\\\nüîç Likely causes:\\\")\\n    print(\\\"  - API keys not configured correctly\\\")\\n    print(\\\"  - Missing dependencies\\\")\\n    print(\\\"  - Network connectivity issues\\\")\\n    print(\\\"  - API rate limits exceeded\\\")\\n\\n# Generate summary report\\nprint(f\\\"\\\\n\\\" + \\\"=\\\" * 80)\\nprint(\\\"üìã PROJECT SUMMARY\\\")\\nprint(\\\"=\\\" * 80)\\nprint(\\\"\\\"\\\"\\nüéØ JOB TREND ANALYZER - COMPLETE IMPLEMENTATION\\n\\n‚úÖ IMPLEMENTED FEATURES:\\n‚Ä¢ Text preprocessing and cleaning\\n‚Ä¢ N-gram extraction with TF-IDF scoring\\n‚Ä¢ Embedding generation using Together AI (m2-bert)\\n‚Ä¢ K-means clustering for skill grouping\\n‚Ä¢ LLM-powered analysis using Google Gemini\\n‚Ä¢ LangChain tool integration\\n‚Ä¢ End-to-end pipeline automation\\n\\nüîß TECHNICAL COMPONENTS:\\n‚Ä¢ Modular architecture with clear separation of concerns\\n‚Ä¢ Error handling and fallback mechanisms\\n‚Ä¢ Performance monitoring and metrics\\n‚Ä¢ Scalable design for larger datasets\\n\\nüìä ANALYSIS CAPABILITIES:\\n‚Ä¢ Skill trend identification\\n‚Ä¢ Technology cluster discovery\\n‚Ä¢ Market insight generation\\n‚Ä¢ Career recommendation system\\n‚Ä¢ Automated report generation\\n\\nüöÄ NEXT STEPS:\\n1. Deploy as a web application using Streamlit\\n2. Add real-time job scraping capabilities\\n3. Implement time-series trend analysis\\n4. Create interactive visualizations\\n5. Build API endpoints for integration\\n6. Add more sophisticated NLP models\\n7. Implement caching for better performance\\n\\nüí° USAGE:\\n- For job seekers: Identify trending skills and career paths\\n- For employers: Understand market demands and skill gaps\\n- For educators: Align curriculum with industry needs\\n- For researchers: Analyze job market evolution\\n\\\"\\\"\\\")\\n\\nprint(f\\\"\\\\nüéâ Demo completed successfully!\\\")\\nprint(f\\\"üìö Check the generated reports and visualizations above.\\\")\\nprint(f\\\"üîó All components are now ready for production use.\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
