{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c6ee31",
   "metadata": {},
   "source": [
    "# 📊 Job Trend Analyzer - Demo Notebook\n",
    "\n",
    "Dự án phân tích xu hướng thị trường việc làm bằng cách kết hợp **n-gram + embedding + Gemini LLM Agent**, được triển khai chuyên nghiệp theo kiến trúc LangChain.\n",
    "\n",
    "## 🎯 Mục tiêu\n",
    "- Phân tích job descriptions từ thị trường việc làm\n",
    "- Trích xuất các kỹ năng và công nghệ trending\n",
    "- Gom cụm các kỹ năng tương tự\n",
    "- Sử dụng AI để phân tích xu hướng và đưa ra insights\n",
    "\n",
    "## 🔄 Luồng xử lý\n",
    "```\n",
    "Job Descriptions → Text Cleaning → N-gram Extraction → Embedding → Clustering → LLM Analysis → Trend Report\n",
    "```\n",
    "\n",
    "## ⚙️ Công nghệ sử dụng\n",
    "- **Text Processing**: NLTK, scikit-learn\n",
    "- **Embeddings**: Together AI (m2-bert-80M-32k-retrieval)\n",
    "- **Clustering**: scikit-learn KMeans\n",
    "- **LLM Agent**: Google Gemini Pro\n",
    "- **Orchestrator**: LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb702575",
   "metadata": {},
   "source": [
    "## 1. 🔧 Setup Environment and Import Libraries\n",
    "\n",
    "Đầu tiên, chúng ta sẽ cài đặt và import tất cả các thư viện cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if not installed)\n",
    "# !pip install together scikit-learn langchain google-generativeai sentence-transformers\n",
    "# !pip install pandas numpy nltk python-dotenv plotly\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project src to path\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.append('../src')\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "print(\"✅ Environment setup completed!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"⚠️ Plotly not available. Using matplotlib for visualization.\")\n",
    "\n",
    "# Import ML libraries\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    SKLEARN_AVAILABLE = True\n",
    "    print(\"✅ Scikit-learn imported successfully\")\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"❌ Scikit-learn not available\")\n",
    "\n",
    "# Import NLP libraries\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"✅ NLTK imported successfully\")\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"❌ NLTK not available\")\n",
    "\n",
    "# Import API clients\n",
    "try:\n",
    "    from together import Together\n",
    "    TOGETHER_AVAILABLE = True\n",
    "    print(\"✅ Together AI client imported successfully\")\n",
    "except ImportError:\n",
    "    TOGETHER_AVAILABLE = False\n",
    "    print(\"❌ Together AI client not available\")\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_AVAILABLE = True\n",
    "    print(\"✅ Google Gemini AI imported successfully\")\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    print(\"❌ Google Gemini AI not available\")\n",
    "\n",
    "try:\n",
    "    from langchain.tools import tool\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "    print(\"✅ LangChain imported successfully\")\n",
    "except ImportError:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "    print(\"❌ LangChain not available\")\n",
    "\n",
    "print(\"\\n📊 Import Summary:\")\n",
    "print(f\"- Scikit-learn: {'✅' if SKLEARN_AVAILABLE else '❌'}\")\n",
    "print(f\"- NLTK: {'✅' if NLTK_AVAILABLE else '❌'}\")\n",
    "print(f\"- Together AI: {'✅' if TOGETHER_AVAILABLE else '❌'}\")\n",
    "print(f\"- Gemini AI: {'✅' if GEMINI_AVAILABLE else '❌'}\")\n",
    "print(f\"- LangChain: {'✅' if LANGCHAIN_AVAILABLE else '❌'}\")\n",
    "print(f\"- Plotly: {'✅' if PLOTLY_AVAILABLE else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f865776",
   "metadata": {},
   "source": [
    "## 2. 🔑 Configure API Keys and Settings\n",
    "\n",
    "Thiết lập cấu hình cho các API keys và parameters cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"⚠️ python-dotenv not available. Set environment variables manually.\")\n",
    "\n",
    "# Configuration class\n",
    "class Config:\n",
    "    \"\"\"Configuration for the Job Trend Analyzer\"\"\"\n",
    "    \n",
    "    # API Keys (load from environment or set manually)\n",
    "    TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\", \"your_together_api_key_here\")\n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"your_gemini_api_key_here\")\n",
    "    \n",
    "    # Model configurations\n",
    "    EMBEDDING_MODEL = \"togethercomputer/m2-bert-80M-32k-retrieval\"\n",
    "    LLM_MODEL = \"gemini-pro\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    NGRAM_RANGE = (1, 3)  # Unigrams to trigrams\n",
    "    TOP_K_NGRAMS = 50\n",
    "    N_CLUSTERS = 8\n",
    "    MIN_WORD_LENGTH = 2\n",
    "    \n",
    "    # API parameters\n",
    "    MAX_RETRIES = 3\n",
    "    RETRY_DELAY = 1.0\n",
    "    BATCH_SIZE = 10\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Validate API keys\n",
    "print(\"🔍 Checking API Configuration:\")\n",
    "print(f\"- Together API Key: {'✅ Set' if config.TOGETHER_API_KEY != 'your_together_api_key_here' else '❌ Not set'}\")\n",
    "print(f\"- Gemini API Key: {'✅ Set' if config.GEMINI_API_KEY != 'your_gemini_api_key_here' else '❌ Not set'}\")\n",
    "\n",
    "if config.TOGETHER_API_KEY == \"your_together_api_key_here\":\n",
    "    print(\"\\n⚠️ Please set your Together API key:\")\n",
    "    print(\"   1. Get API key from: https://api.together.xyz/\")\n",
    "    print(\"   2. Set environment variable: TOGETHER_API_KEY=your_key\")\n",
    "    print(\"   3. Or update the Config class above\")\n",
    "\n",
    "if config.GEMINI_API_KEY == \"your_gemini_api_key_here\":\n",
    "    print(\"\\n⚠️ Please set your Gemini API key:\")\n",
    "    print(\"   1. Get API key from: https://makersuite.google.com/app/apikey\")\n",
    "    print(\"   2. Set environment variable: GEMINI_API_KEY=your_key\")\n",
    "    print(\"   3. Or update the Config class above\")\n",
    "\n",
    "print(f\"\\n📋 Processing Configuration:\")\n",
    "print(f\"- Embedding Model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"- LLM Model: {config.LLM_MODEL}\")\n",
    "print(f\"- N-gram Range: {config.NGRAM_RANGE}\")\n",
    "print(f\"- Top K N-grams: {config.TOP_K_NGRAMS}\")\n",
    "print(f\"- Number of Clusters: {config.N_CLUSTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac2737",
   "metadata": {},
   "source": [
    "## 3. 📝 Text Preprocessing Module\n",
    "\n",
    "Xây dựng module để làm sạch và xử lý văn bản job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b51076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data if needed\n",
    "if NLTK_AVAILABLE:\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"📥 Downloading NLTK data...\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and preprocess job description text\n",
    "    \n",
    "    Args:\n",
    "        text: Raw job description text\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs and email addresses\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'[\\+]?[1-9]?[0-9]{7,15}', '', text)\n",
    "    \n",
    "    # Remove special characters but keep letters, numbers, and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove job-specific stopwords\n",
    "    job_stopwords = {\n",
    "        'job', 'position', 'role', 'candidate', 'applicant', 'experience',\n",
    "        'work', 'company', 'team', 'office', 'location', 'salary',\n",
    "        'benefit', 'requirement', 'qualification', 'responsibility',\n",
    "        'opportunity', 'career', 'employment', 'hire', 'hiring', 'year', 'years'\n",
    "    }\n",
    "    \n",
    "    # Get English stopwords\n",
    "    if NLTK_AVAILABLE:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(job_stopwords)\n",
    "        \n",
    "        # Tokenize and filter\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens \n",
    "                 if token.lower() not in stop_words \n",
    "                 and len(token) >= config.MIN_WORD_LENGTH \n",
    "                 and token.isalpha()]\n",
    "    else:\n",
    "        # Simple tokenization if NLTK not available\n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens \n",
    "                 if token.lower() not in job_stopwords \n",
    "                 and len(token) >= config.MIN_WORD_LENGTH \n",
    "                 and token.isalpha()]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the preprocessing function\n",
    "test_job_desc = \"\"\"\n",
    "We are looking for a Senior Python Developer with 5+ years of experience \n",
    "in machine learning and data science. The candidate should have expertise \n",
    "in TensorFlow, PyTorch, and scikit-learn. \n",
    "\n",
    "Requirements:\n",
    "- Bachelor's degree in Computer Science\n",
    "- Experience with AWS/GCP cloud platforms\n",
    "- Strong knowledge of SQL and NoSQL databases\n",
    "- Excellent communication skills\n",
    "\n",
    "Salary: $120,000 - $150,000 per year\n",
    "Location: San Francisco, CA\n",
    "Email: jobs@company.com\n",
    "Phone: (555) 123-4567\n",
    "Visit our website: https://company.com\n",
    "\"\"\"\n",
    "\n",
    "print(\"🧪 Testing Text Preprocessing:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Original text:\")\n",
    "print(test_job_desc[:200] + \"...\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Cleaned text:\")\n",
    "cleaned = clean_text(test_job_desc)\n",
    "print(cleaned)\n",
    "print(f\"\\nLength reduction: {len(test_job_desc)} → {len(cleaned)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c6586",
   "metadata": {},
   "source": [
    "## 4. 🔤 N-gram Extraction Implementation\n",
    "\n",
    "Triển khai trích xuất n-gram từ văn bản đã được làm sạch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97551cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(texts: List[str], ngram_range: Tuple[int, int] = (1, 3), \n",
    "               top_k: int = 50, use_tfidf: bool = True) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract n-grams from a list of texts using sklearn\n",
    "    \n",
    "    Args:\n",
    "        texts: List of preprocessed texts\n",
    "        ngram_range: Range of n-gram sizes (min_n, max_n)\n",
    "        top_k: Number of top n-grams to return\n",
    "        use_tfidf: Whether to use TF-IDF or simple count\n",
    "        \n",
    "    Returns:\n",
    "        List of (ngram, score) tuples sorted by score\n",
    "    \"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        print(\"❌ Scikit-learn not available. Cannot extract n-grams.\")\n",
    "        return []\n",
    "    \n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    # Filter out empty texts\n",
    "    texts = [text for text in texts if text.strip()]\n",
    "    \n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Choose vectorizer\n",
    "        if use_tfidf:\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                ngram_range=ngram_range,\n",
    "                max_features=top_k * 2,  # Get more features for better selection\n",
    "                min_df=2,  # Must appear in at least 2 documents\n",
    "                max_df=0.8,  # Must not appear in more than 80% of documents\n",
    "                lowercase=True,\n",
    "                token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b'  # Only alphabetic tokens\n",
    "            )\n",
    "        else:\n",
    "            vectorizer = CountVectorizer(\n",
    "                ngram_range=ngram_range,\n",
    "                max_features=top_k * 2,\n",
    "                min_df=2,\n",
    "                max_df=0.8,\n",
    "                lowercase=True,\n",
    "                token_pattern=r'\\b[a-zA-Z][a-zA-Z]+\\b'\n",
    "            )\n",
    "        \n",
    "        # Fit and transform\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Calculate scores (sum across documents)\n",
    "        scores = X.sum(axis=0).A1\n",
    "        \n",
    "        # Create list of (ngram, score) tuples\n",
    "        ngram_scores = list(zip(feature_names, scores))\n",
    "        \n",
    "        # Sort by score (descending) and take top_k\n",
    "        ngram_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return ngram_scores[:top_k]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting n-grams: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test n-gram extraction with sample data\n",
    "sample_texts = [\n",
    "    \"python developer machine learning tensorflow pytorch\",\n",
    "    \"java backend spring boot microservices aws\",\n",
    "    \"javascript react frontend angular html css\",\n",
    "    \"data scientist python pandas numpy machine learning\",\n",
    "    \"devops engineer kubernetes docker aws cloud\",\n",
    "    \"fullstack developer python javascript react postgresql\",\n",
    "    \"backend engineer java spring boot rest api\",\n",
    "    \"ai engineer deep learning tensorflow python\",\n",
    "    \"cloud architect aws azure kubernetes microservices\",\n",
    "    \"data engineer python sql spark hadoop\"\n",
    "]\n",
    "\n",
    "# Clean the sample texts\n",
    "cleaned_texts = [clean_text(text) for text in sample_texts]\n",
    "\n",
    "print(\"🧪 Testing N-gram Extraction:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample texts: {len(sample_texts)}\")\n",
    "print(f\"Cleaned texts: {len(cleaned_texts)}\")\n",
    "\n",
    "# Extract n-grams\n",
    "ngrams = get_ngrams(cleaned_texts, \n",
    "                   ngram_range=config.NGRAM_RANGE, \n",
    "                   top_k=20, \n",
    "                   use_tfidf=True)\n",
    "\n",
    "print(f\"\\n📊 Top 20 N-grams (TF-IDF):\")\n",
    "for i, (ngram, score) in enumerate(ngrams):\n",
    "    print(f\"{i+1:2d}. {ngram:<25} (score: {score:.3f})\")\n",
    "\n",
    "# Compare with count-based extraction\n",
    "ngrams_count = get_ngrams(cleaned_texts, \n",
    "                         ngram_range=config.NGRAM_RANGE, \n",
    "                         top_k=10, \n",
    "                         use_tfidf=False)\n",
    "\n",
    "print(f\"\\n📊 Top 10 N-grams (Count):\")\n",
    "for i, (ngram, score) in enumerate(ngrams_count):\n",
    "    print(f\"{i+1:2d}. {ngram:<25} (count: {score:.0f})\")\n",
    "\n",
    "# Analyze n-gram lengths\n",
    "if ngrams:\n",
    "    unigrams = [(n, s) for n, s in ngrams if len(n.split()) == 1]\n",
    "    bigrams = [(n, s) for n, s in ngrams if len(n.split()) == 2] \n",
    "    trigrams = [(n, s) for n, s in ngrams if len(n.split()) == 3]\n",
    "    \n",
    "    print(f\"\\n📈 N-gram Distribution:\")\n",
    "    print(f\"- Unigrams: {len(unigrams)}\")\n",
    "    print(f\"- Bigrams: {len(bigrams)}\")\n",
    "    print(f\"- Trigrams: {len(trigrams)}\")\n",
    "    \n",
    "    if bigrams:\n",
    "        print(f\"\\nTop Bigrams: {', '.join([n for n, s in bigrams[:5]])}\")\n",
    "    if trigrams:\n",
    "        print(f\"Top Trigrams: {', '.join([n for n, s in trigrams[:3]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad0c4f",
   "metadata": {},
   "source": [
    "## 5. 🧠 Embedding Generation with Together API\n",
    "\n",
    "Tạo vector embeddings cho các n-gram sử dụng Together AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(phrases: List[str], api_key: str = None) -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of phrases using Together API\n",
    "    \n",
    "    Args:\n",
    "        phrases: List of phrases to embed\n",
    "        api_key: Together API key (uses config if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        List of (phrase, embedding) tuples\n",
    "    \"\"\"\n",
    "    if not TOGETHER_AVAILABLE:\n",
    "        print(\"❌ Together AI not available. Cannot generate embeddings.\")\n",
    "        return []\n",
    "    \n",
    "    api_key = api_key or config.TOGETHER_API_KEY\n",
    "    \n",
    "    if api_key == \"your_together_api_key_here\":\n",
    "        print(\"❌ Please set your Together API key first.\")\n",
    "        return []\n",
    "    \n",
    "    if not phrases:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Initialize Together client\n",
    "        client = Together(api_key=api_key)\n",
    "        \n",
    "        result = []\n",
    "        print(f\"🔄 Creating embeddings for {len(phrases)} phrases...\")\n",
    "        \n",
    "        for i, phrase in enumerate(phrases):\n",
    "            try:\n",
    "                response = client.embeddings.create(\n",
    "                    model=config.EMBEDDING_MODEL,\n",
    "                    input=phrase\n",
    "                )\n",
    "                \n",
    "                if response.data and len(response.data) > 0:\n",
    "                    embedding = response.data[0].embedding\n",
    "                    result.append((phrase, embedding))\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 5 == 0:\n",
    "                    print(f\"  Progress: {i + 1}/{len(phrases)} embeddings created\")\n",
    "                \n",
    "                # Small delay to be respectful to API\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to embed phrase '{phrase}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ Successfully created {len(result)}/{len(phrases)} embeddings\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "# Create sample embeddings (if API key is available)\n",
    "if config.TOGETHER_API_KEY != \"your_together_api_key_here\" and TOGETHER_AVAILABLE:\n",
    "    print(\"🧪 Testing Embedding Generation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use top 10 n-grams for testing\n",
    "    test_phrases = [ngram for ngram, score in ngrams[:10]]\n",
    "    print(f\"Test phrases: {test_phrases}\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = get_embeddings(test_phrases)\n",
    "    \n",
    "    if embeddings:\n",
    "        print(f\"\\n📊 Embedding Results:\")\n",
    "        print(f\"- Number of embeddings: {len(embeddings)}\")\n",
    "        print(f\"- Embedding dimension: {len(embeddings[0][1]) if embeddings else 0}\")\n",
    "        \n",
    "        # Show first few embeddings (truncated)\n",
    "        for i, (phrase, embedding) in enumerate(embeddings[:3]):\n",
    "            embedding_preview = embedding[:5] + ['...'] if len(embedding) > 5 else embedding\n",
    "            print(f\"  {phrase}: {embedding_preview}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        if embeddings:\n",
    "            all_embeddings = [emb for _, emb in embeddings]\n",
    "            embedding_matrix = np.array(all_embeddings)\n",
    "            \n",
    "            print(f\"\\n📈 Embedding Statistics:\")\n",
    "            print(f\"- Mean magnitude: {np.mean(np.linalg.norm(embedding_matrix, axis=1)):.3f}\")\n",
    "            print(f\"- Std magnitude: {np.std(np.linalg.norm(embedding_matrix, axis=1)):.3f}\")\n",
    "            print(f\"- Dimension: {embedding_matrix.shape[1]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Skipping embedding generation - API key not available\")\n",
    "    print(\"Setting up mock embeddings for demonstration...\")\n",
    "    \n",
    "    # Create mock embeddings for testing without API\n",
    "    np.random.seed(42)\n",
    "    test_phrases = [ngram for ngram, score in ngrams[:10]]\n",
    "    embeddings = []\n",
    "    \n",
    "    for phrase in test_phrases:\n",
    "        # Create mock embedding (768 dimensions like m2-bert)\n",
    "        mock_embedding = np.random.normal(0, 1, 768).tolist()\n",
    "        embeddings.append((phrase, mock_embedding))\n",
    "    \n",
    "    print(f\"✅ Created {len(embeddings)} mock embeddings for testing\")\n",
    "    print(f\"   Embedding dimension: {len(embeddings[0][1]) if embeddings else 0}\")\n",
    "\n",
    "# Store embeddings for next steps\n",
    "embedding_data = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed92e62",
   "metadata": {},
   "source": [
    "## 6. 📊 Clustering Implementation\n",
    "\n",
    "Gom cụm các embeddings để tìm ra các nhóm kỹ năng tương tự."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc55192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings(embeddings: List[Tuple[str, List[float]]], \n",
    "                      n_clusters: int = 5) -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Cluster embeddings using KMeans algorithm\n",
    "    \n",
    "    Args:\n",
    "        embeddings: List of (phrase, embedding) tuples\n",
    "        n_clusters: Number of clusters to create\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping cluster IDs to lists of phrases\n",
    "    \"\"\"\n",
    "    if not SKLEARN_AVAILABLE:\n",
    "        print(\"❌ Scikit-learn not available. Cannot perform clustering.\")\n",
    "        return {}\n",
    "    \n",
    "    if not embeddings or len(embeddings) < n_clusters:\n",
    "        print(f\"⚠️ Not enough embeddings ({len(embeddings)}) for {n_clusters} clusters\")\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Extract phrases and vectors\n",
    "        phrases = [phrase for phrase, _ in embeddings]\n",
    "        vectors = np.array([embedding for _, embedding in embeddings])\n",
    "        \n",
    "        print(f\"🔄 Clustering {len(phrases)} phrases into {n_clusters} clusters...\")\n",
    "        \n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(vectors)\n",
    "        \n",
    "        # Organize results into clusters\n",
    "        clusters = {i: [] for i in range(n_clusters)}\n",
    "        for phrase, label in zip(phrases, labels):\n",
    "            clusters[label].append(phrase)\n",
    "        \n",
    "        # Calculate clustering quality metrics\n",
    "        silhouette_avg = silhouette_score(vectors, labels)\n",
    "        \n",
    "        print(f\"✅ Clustering completed!\")\n",
    "        print(f\"   Silhouette Score: {silhouette_avg:.3f}\")\n",
    "        print(f\"   Inertia: {kmeans.inertia_:.3f}\")\n",
    "        \n",
    "        # Display cluster sizes\n",
    "        cluster_sizes = [len(clusters[i]) for i in range(n_clusters)]\n",
    "        print(f\"   Cluster sizes: {cluster_sizes}\")\n",
    "        \n",
    "        return clusters\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during clustering: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Perform clustering on the embeddings\n",
    "if embedding_data:\n",
    "    print(\"🧪 Testing Clustering:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Cluster the embeddings\n",
    "    clusters = cluster_embeddings(embedding_data, n_clusters=min(5, len(embedding_data)))\n",
    "    \n",
    "    if clusters:\n",
    "        print(f\"\\n📊 Clustering Results:\")\n",
    "        print(f\"Number of clusters: {len(clusters)}\")\n",
    "        \n",
    "        # Display each cluster\n",
    "        for cluster_id, phrases in clusters.items():\n",
    "            print(f\"\\n🏷️ Cluster {cluster_id + 1} ({len(phrases)} items):\")\n",
    "            for phrase in phrases:\n",
    "                print(f\"   • {phrase}\")\n",
    "    \n",
    "    # Visualize clusters if possible\n",
    "    if SKLEARN_AVAILABLE and embedding_data and len(embedding_data) > 1:\n",
    "        print(f\"\\n📈 Creating 2D visualization...\")\n",
    "        \n",
    "        # Extract vectors for PCA\n",
    "        vectors = np.array([embedding for _, embedding in embedding_data])\n",
    "        phrases = [phrase for phrase, _ in embedding_data]\n",
    "        \n",
    "        # Reduce to 2D using PCA\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        vectors_2d = pca.fit_transform(vectors)\n",
    "        \n",
    "        # Get cluster labels\n",
    "        if clusters:\n",
    "            labels = []\n",
    "            for phrase in phrases:\n",
    "                for cluster_id, cluster_phrases in clusters.items():\n",
    "                    if phrase in cluster_phrases:\n",
    "                        labels.append(cluster_id)\n",
    "                        break\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Plot points with cluster colors\n",
    "            for cluster_id in range(len(clusters)):\n",
    "                cluster_points = vectors_2d[np.array(labels) == cluster_id]\n",
    "                if len(cluster_points) > 0:\n",
    "                    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "                              label=f'Cluster {cluster_id + 1}', alpha=0.7, s=100)\n",
    "            \n",
    "            # Add labels for points\n",
    "            for i, phrase in enumerate(phrases):\n",
    "                plt.annotate(phrase, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                           xytext=(5, 5), textcoords='offset points', \n",
    "                           fontsize=8, alpha=0.7)\n",
    "            \n",
    "            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "            plt.title('2D Visualization of Skill Clusters (PCA)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"✅ 2D visualization created\")\n",
    "            print(f\"   Total variance explained: {sum(pca.explained_variance_ratio_):.1%}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No embeddings available for clustering\")\n",
    "    clusters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081badb1",
   "metadata": {},
   "source": [
    "## 7. 🤖 LLM Agent Integration with Gemini\n",
    "\n",
    "Sử dụng Google Gemini để phân tích các cụm và tạo insights về xu hướng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50da479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(clusters: Dict[int, List[str]], api_key: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Analyze clusters using Google Gemini to generate trend insights\n",
    "    \n",
    "    Args:\n",
    "        clusters: Dictionary mapping cluster IDs to lists of phrases\n",
    "        api_key: Gemini API key (uses config if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Analysis text from Gemini\n",
    "    \"\"\"\n",
    "    if not GEMINI_AVAILABLE:\n",
    "        print(\"❌ Google Gemini AI not available. Cannot perform analysis.\")\n",
    "        return \"Gemini AI not available for analysis.\"\n",
    "    \n",
    "    api_key = api_key or config.GEMINI_API_KEY\n",
    "    \n",
    "    if api_key == \"your_gemini_api_key_here\":\n",
    "        print(\"❌ Please set your Gemini API key first.\")\n",
    "        return \"Gemini API key not configured.\"\n",
    "    \n",
    "    if not clusters:\n",
    "        return \"No clusters available for analysis.\"\n",
    "    \n",
    "    try:\n",
    "        # Configure Gemini\n",
    "        genai.configure(api_key=api_key)\n",
    "        model = genai.GenerativeModel(config.LLM_MODEL)\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = \\\"\\\"\\\"Bạn là một chuyên gia phân tích thị trường việc làm công nghệ. \n",
    "Dưới đây là các nhóm kỹ năng/công nghệ đã được gom cụm từ job descriptions:\n",
    "\n",
    "\\\"\\\"\\\"\\n        \n",
    "        for cluster_id, phrases in clusters.items():\n",
    "            prompt += f\\\"\\\\nNhóm {cluster_id + 1}: {', '.join(phrases[:10])}\\\"\\n            if len(phrases) > 10:\\n                prompt += f\\\" (và {len(phrases) - 10} kỹ năng khác)\\\"\\n        \\n        prompt += \\\"\\\"\\\"\\\\n\\\\nHãy phân tích và đưa ra những nhận định sâu sắc về:\\\\n1. Xu hướng tăng trưởng của từng nhóm kỹ năng\\\\n2. Những công nghệ/kỹ năng hot nhất hiện tại\\\\n3. Những kỹ năng đang suy giảm (nếu có)\\\\n4. Dự đoán xu hướng trong 1-2 năm tới\\\\n5. Lời khuyên cho người tìm việc trong ngành IT\\\\n\\\\nHãy trả lời một cách chi tiết và chuyên nghiệp.\\\"\\\"\\\"\\n        \\n        print(\\\"🔄 Analyzing clusters with Gemini AI...\\\")\\n        \\n        # Generate response\\n        response = model.generate_content(\\n            prompt,\\n            generation_config={\\n                'temperature': 0.3,\\n                'max_output_tokens': 1000,\\n                'top_p': 0.8,\\n                'top_k': 40\\n            }\\n        )\\n        \\n        if response.text:\\n            print(\\\"✅ Analysis completed!\\\")\\n            return response.text\\n        else:\\n            return \\\"No analysis generated.\\\"\\n            \\n    except Exception as e:\\n        print(f\\\"❌ Error during analysis: {e}\\\")\\n        return f\\\"Error during analysis: {e}\\\"\\n\\n# Perform cluster analysis\\nif clusters and config.GEMINI_API_KEY != \\\"your_gemini_api_key_here\\\" and GEMINI_AVAILABLE:\\n    print(\\\"🧪 Testing LLM Analysis:\\\")\\n    print(\\\"=\\\" * 50)\\n    \\n    analysis_result = analyze_clusters(clusters)\\n    \\n    print(\\\"\\\\n🤖 AI Analysis Results:\\\")\\n    print(\\\"=\\\" * 50)\\n    print(analysis_result)\\n    \\nelse:\\n    print(\\\"⚠️ Skipping LLM analysis - API key not available or no clusters\\\")\\n    print(\\\"Generating mock analysis for demonstration...\\\")\\n    \\n    analysis_result = \\\"\\\"\\\"🔍 PHÂN TÍCH XU HƯỚNG THỊ TRƯỜNG VIỆC LÀM IT (Mock Analysis)\\n\\n📈 XU HƯỚNG TĂNG TRƯỞNG:\\n• Nhóm AI/ML: Python, machine learning, deep learning đang có xu hướng tăng mạnh\\n• Nhóm Cloud: AWS, Docker, Kubernetes tiếp tục là những kỹ năng hot\\n• Nhóm Frontend: React, JavaScript vẫn duy trì sức nóng\\n\\n🔥 KỸ NĂNG HOT NHẤT:\\n1. Python - Ngôn ngữ đa năng, ứng dụng rộng rãi\\n2. Machine Learning - Xu hướng AI đang bùng nổ\\n3. Cloud Technologies - Chuyển đổi số đẩy nhu cầu cloud\\n4. React/JavaScript - Frontend development vẫn rất cần\\n\\n📉 KỸ NĂNG ĐANG SUY GIẢM:\\n• Các công nghệ legacy như VB.NET, Flash\\n• Một số framework cũ đang được thay thế\\n\\n🔮 Dự ĐOÁN 1-2 NĂM TỚI:\\n• AI/ML sẽ tiếp tục tăng trưởng mạnh\\n• Cloud-native development sẽ trở thành chuẩn\\n• Low-code/No-code platforms sẽ phát triển\\n• DevOps và automation càng quan trọng\\n\\n💡 LỜI KHUYÊN:\\n1. Tập trung học Python và machine learning\\n2. Nắm vững cloud platforms (AWS/Azure)\\n3. Phát triển kỹ năng full-stack\\n4. Luôn cập nhật công nghệ mới\\\"\\\"\\\"\\n    \\n    print(\\\"\\\\n🤖 Mock Analysis Results:\\\")\\n    print(\\\"=\\\" * 50)\\n    print(analysis_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05330432",
   "metadata": {},
   "source": [
    "## 8. ⚙️ LangChain Tool Creation\n",
    "\n",
    "Tích hợp toàn bộ pipeline vào một LangChain tool để sử dụng dễ dàng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce54c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LANGCHAIN_AVAILABLE:\\n    @tool\\n    def analyze_job_trend(texts: List[str]) -> str:\\n        \\\"\\\"\\\"Analyze job market trends from job descriptions.\\n        \\n        Args:\\n            texts: List of job description texts\\n            \\n        Returns:\\n            Comprehensive trend analysis report\\n        \\\"\\\"\\\"\\n        try:\\n            print(\\\"🚀 Starting Job Trend Analysis Pipeline...\\\")\\n            \\n            # Step 1: Clean texts\\n            print(\\\"1️⃣ Cleaning texts...\\\")\\n            cleaned = [clean_text(t) for t in texts if t.strip()]\\n            \\n            if not cleaned:\\n                return \\\"No valid texts provided for analysis.\\\"\\n            \\n            print(f\\\"   Processed {len(cleaned)} texts\\\")\\n            \\n            # Step 2: Extract n-grams\\n            print(\\\"2️⃣ Extracting n-grams...\\\")\\n            ngrams = get_ngrams(cleaned, \\n                               ngram_range=config.NGRAM_RANGE, \\n                               top_k=config.TOP_K_NGRAMS)\\n            \\n            if not ngrams:\\n                return \\\"No n-grams could be extracted from the texts.\\\"\\n                \\n            print(f\\\"   Extracted {len(ngrams)} n-grams\\\")\\n            \\n            # Step 3: Create embeddings\\n            print(\\\"3️⃣ Creating embeddings...\\\")\\n            phrases = [g[0] for g in ngrams[:20]]  # Limit for demo\\n            embeddings = get_embeddings(phrases)\\n            \\n            if not embeddings:\\n                return \\\"No embeddings could be created. Check API configuration.\\\"\\n                \\n            print(f\\\"   Created {len(embeddings)} embeddings\\\")\\n            \\n            # Step 4: Cluster embeddings\\n            print(\\\"4️⃣ Clustering embeddings...\\\")\\n            clusters = cluster_embeddings(embeddings, \\n                                        n_clusters=min(config.N_CLUSTERS, len(embeddings)))\\n            \\n            if not clusters:\\n                return \\\"Clustering failed. Not enough data.\\\"\\n                \\n            print(f\\\"   Created {len(clusters)} clusters\\\")\\n            \\n            # Step 5: Analyze with LLM\\n            print(\\\"5️⃣ Analyzing with AI...\\\")\\n            analysis = analyze_clusters(clusters)\\n            \\n            # Compile final report\\n            report = f\\\"\\\"\\\"# 📊 JOB TREND ANALYSIS REPORT\\n\\n## 📈 Data Summary\\n- Job descriptions analyzed: {len(texts)}\\n- Valid texts processed: {len(cleaned)}\\n- N-grams extracted: {len(ngrams)}\\n- Skill clusters identified: {len(clusters)}\\n\\n## 🔝 Top Trending Skills\\n\\\"\\\"\\\"\\n            \\n            for i, (ngram, score) in enumerate(ngrams[:10]):\\n                report += f\\\"{i+1}. {ngram} (score: {score:.2f})\\\\n\\\"\\n            \\n            report += f\\\"\\\\n## 🏷️ Skill Clusters\\\\n\\\"\\n            for cluster_id, phrases in clusters.items():\\n                report += f\\\"\\\\n**Cluster {cluster_id + 1}:** {', '.join(phrases[:5])}\\\"\\n                if len(phrases) > 5:\\n                    report += f\\\" (và {len(phrases) - 5} kỹ năng khác)\\\"\\n                report += \\\"\\\\n\\\"\\n            \\n            report += f\\\"\\\\n## 🤖 AI Analysis\\\\n{analysis}\\\"\\n            \\n            print(\\\"✅ Pipeline completed successfully!\\\")\\n            return report\\n            \\n        except Exception as e:\\n            error_msg = f\\\"❌ Pipeline failed: {str(e)}\\\"\\n            print(error_msg)\\n            return error_msg\\n    \\n    print(\\\"✅ LangChain tool 'analyze_job_trend' created successfully!\\\")\\n    print(\\\"Usage: result = analyze_job_trend.invoke({'texts': your_job_descriptions})\\\")\\n    \\nelse:\\n    print(\\\"⚠️ LangChain not available. Tool creation skipped.\\\")\\n    \\n    # Create a simple wrapper function instead\\n    def analyze_job_trend_simple(texts: List[str]) -> str:\\n        \\\"\\\"\\\"Simple version of the job trend analyzer without LangChain\\\"\\\"\\\"\\n        try:\\n            # Run the pipeline steps\\n            cleaned = [clean_text(t) for t in texts if t.strip()]\\n            ngrams = get_ngrams(cleaned, ngram_range=config.NGRAM_RANGE, top_k=20)\\n            phrases = [g[0] for g in ngrams[:10]]\\n            embeddings = get_embeddings(phrases) if phrases else []\\n            clusters = cluster_embeddings(embeddings, n_clusters=min(5, len(embeddings))) if embeddings else {}\\n            analysis = analyze_clusters(clusters) if clusters else \\\"No clusters available for analysis.\\\"\\n            \\n            # Create simple report\\n            report = f\\\"\\\"\\\"JOB TREND ANALYSIS\\\\n\\\\nProcessed {len(cleaned)} texts\\\\nTop skills: {', '.join([n for n, s in ngrams[:5]])}\\\\n\\\\nAnalysis:\\\\n{analysis}\\\"\\\"\\\"\\n            \\n            return report\\n            \\n        except Exception as e:\\n            return f\\\"Analysis failed: {str(e)}\\\"\\n    \\n    print(\\\"✅ Simple analyzer function created as fallback\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1978f7",
   "metadata": {},
   "source": [
    "## 9. 🔧 Pipeline Integration Testing\n",
    "\n",
    "Kiểm tra hoạt động của toàn bộ pipeline với dữ liệu mẫu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c918f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the integrated pipeline with small dataset\\ntest_job_descriptions = [\\n    \\\"Senior Python Developer with machine learning experience using TensorFlow and PyTorch\\\",\\n    \\\"Java Backend Engineer working with Spring Boot and microservices architecture\\\", \\n    \\\"Frontend Developer specializing in React, Angular, and modern JavaScript frameworks\\\",\\n    \\\"Data Scientist proficient in Python, pandas, scikit-learn, and statistical analysis\\\",\\n    \\\"DevOps Engineer experienced with AWS, Kubernetes, Docker, and CI/CD pipelines\\\"\\n]\\n\\nprint(\\\"🧪 Testing Complete Pipeline Integration:\\\")\\nprint(\\\"=\\\" * 60)\\nprint(f\\\"Test dataset: {len(test_job_descriptions)} job descriptions\\\")\\n\\n# Test individual components first\\nprint(\\\"\\\\n🔍 Component Testing:\\\")\\nprint(\\\"-\\\" * 30)\\n\\n# Test preprocessing\\ntest_cleaned = [clean_text(desc) for desc in test_job_descriptions]\\nprint(f\\\"✅ Preprocessing: {len(test_cleaned)} texts cleaned\\\")\\n\\n# Test n-gram extraction\\ntest_ngrams = get_ngrams(test_cleaned, ngram_range=(1, 2), top_k=15)\\nprint(f\\\"✅ N-gram extraction: {len(test_ngrams)} n-grams extracted\\\")\\n\\nif test_ngrams:\\n    print(\\\"   Top 5 n-grams:\\\", [n for n, s in test_ngrams[:5]])\\n\\n# Test component integration\\nprint(\\\"\\\\n🔗 Integration Testing:\\\")\\nprint(\\\"-\\\" * 30)\\n\\nstart_time = time.time()\\n\\ntry:\\n    if LANGCHAIN_AVAILABLE and 'analyze_job_trend' in locals():\\n        print(\\\"Testing LangChain tool...\\\")\\n        result = analyze_job_trend.invoke({\\\"texts\\\": test_job_descriptions})\\n    else:\\n        print(\\\"Testing simple analyzer...\\\")\\n        result = analyze_job_trend_simple(test_job_descriptions)\\n    \\n    execution_time = time.time() - start_time\\n    \\n    print(f\\\"\\\\n⏱️ Execution time: {execution_time:.2f} seconds\\\")\\n    print(f\\\"\\\\n📋 Pipeline Result:\\\")\\n    print(\\\"=\\\" * 60)\\n    print(result)\\n    \\nexcept Exception as e:\\n    print(f\\\"❌ Pipeline test failed: {e}\\\")\\n    print(\\\"This is expected if API keys are not configured\\\")\\n\\n# Performance summary\\nprint(\\\"\\\\n📊 Performance Summary:\\\")\\nprint(\\\"=\\\" * 30)\\nprint(f\\\"- Input texts: {len(test_job_descriptions)}\\\")\\nprint(f\\\"- Processing time: {execution_time:.2f}s\\\" if 'execution_time' in locals() else \\\"- Processing time: N/A\\\")\\nprint(f\\\"- Average time per text: {(execution_time/len(test_job_descriptions)):.2f}s\\\" if 'execution_time' in locals() else \\\"- Average time per text: N/A\\\")\\n\\n# Component availability summary\\nprint(\\\"\\\\n🔧 Component Status:\\\")\\nprint(\\\"=\\\" * 30)\\ncomponents = {\\n    \\\"Text Preprocessing\\\": True,\\n    \\\"N-gram Extraction\\\": SKLEARN_AVAILABLE,\\n    \\\"Embedding Generation\\\": TOGETHER_AVAILABLE and config.TOGETHER_API_KEY != \\\"your_together_api_key_here\\\",\\n    \\\"Clustering\\\": SKLEARN_AVAILABLE,\\n    \\\"LLM Analysis\\\": GEMINI_AVAILABLE and config.GEMINI_API_KEY != \\\"your_gemini_api_key_here\\\",\\n    \\\"LangChain Integration\\\": LANGCHAIN_AVAILABLE\\n}\\n\\nfor component, status in components.items():\\n    status_icon = \\\"✅\\\" if status else \\\"❌\\\"\\n    print(f\\\"{status_icon} {component}\\\")\\n\\nfunctional_components = sum(components.values())\\ntotal_components = len(components)\\nprint(f\\\"\\\\n📈 Overall Status: {functional_components}/{total_components} components functional ({functional_components/total_components*100:.0f}%)\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca34a8b",
   "metadata": {},
   "source": [
    "## 10. 📈 Sample Data Analysis\n",
    "\n",
    "Chạy phân tích trên bộ dữ liệu lớn hơn để thể hiện khả năng thực tế của hệ thống."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended sample dataset for comprehensive analysis\\nsample_job_dataset = [\\n    \\\"Senior Python Developer with 5+ years experience in machine learning, TensorFlow, PyTorch, and scikit-learn. Knowledge of AWS cloud services required.\\\",\\n    \\\"Java Backend Engineer to build scalable microservices using Spring Boot, REST APIs, and MySQL. Docker and Kubernetes experience preferred.\\\",\\n    \\\"Frontend React Developer proficient in JavaScript, TypeScript, Redux, and modern CSS frameworks. Experience with Next.js is a plus.\\\",\\n    \\\"Data Scientist with expertise in Python, pandas, NumPy, statistical analysis, and machine learning algorithms. SQL and data visualization skills required.\\\",\\n    \\\"DevOps Engineer experienced with AWS/Azure, Kubernetes, Docker, CI/CD pipelines, Infrastructure as Code, and monitoring tools.\\\",\\n    \\\"Full Stack Developer using MEAN/MERN stack. Proficiency in Node.js, Express, MongoDB, and Angular/React frameworks required.\\\",\\n    \\\"AI/ML Engineer to develop deep learning models using TensorFlow, PyTorch, computer vision, and natural language processing techniques.\\\",\\n    \\\"Cloud Architect to design scalable solutions on AWS/Azure platforms. Experience with serverless computing and microservices architecture.\\\",\\n    \\\"Mobile Developer creating cross-platform apps with React Native and Flutter. Knowledge of native iOS and Android development preferred.\\\",\\n    \\\"Cybersecurity Analyst with expertise in penetration testing, vulnerability assessment, network security, and incident response procedures.\\\",\\n    \\\"QA Engineer skilled in test automation, Selenium, API testing, and performance testing. Experience with Agile methodologies required.\\\",\\n    \\\"Product Manager with technical background in software development. Experience with user research, data analysis, and product strategy.\\\",\\n    \\\"Database Administrator proficient in MySQL, PostgreSQL, MongoDB, performance optimization, and backup/recovery procedures.\\\",\\n    \\\"UI/UX Designer with strong skills in Figma, Adobe Creative Suite, user research, prototyping, and responsive web design principles.\\\",\\n    \\\"Blockchain Developer experienced with Ethereum, Solidity, smart contracts, DeFi protocols, and Web3 technologies.\\\"\\n]\\n\\nprint(\\\"🚀 COMPREHENSIVE JOB TREND ANALYSIS\\\")\\nprint(\\\"=\\\" * 60)\\nprint(f\\\"Analyzing {len(sample_job_dataset)} diverse job descriptions...\\\")\\nprint(\\\"This represents a realistic job market sample.\\\")\\n\\n# Run comprehensive analysis\\nstart_time = time.time()\\n\\ntry:\\n    # Choose the appropriate analyzer\\n    if LANGCHAIN_AVAILABLE and 'analyze_job_trend' in locals():\\n        print(\\\"\\\\n🔧 Using LangChain-powered analyzer...\\\")\\n        final_result = analyze_job_trend.invoke({\\\"texts\\\": sample_job_dataset})\\n    else:\\n        print(\\\"\\\\n🔧 Using simple analyzer...\\\")\\n        final_result = analyze_job_trend_simple(sample_job_dataset)\\n    \\n    total_time = time.time() - start_time\\n    \\n    print(f\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n    print(\\\"📊 FINAL ANALYSIS RESULTS\\\")\\n    print(\\\"=\\\" * 80)\\n    print(final_result)\\n    \\n    print(f\\\"\\\\n\\\" + \\\"=\\\" * 80)\\n    print(\\\"⏱️ PERFORMANCE METRICS\\\")\\n    print(\\\"=\\\" * 80)\\n    print(f\\\"• Total processing time: {total_time:.2f} seconds\\\")\\n    print(f\\\"• Average time per job description: {total_time/len(sample_job_dataset):.2f} seconds\\\")\\n    print(f\\\"• Dataset size: {len(sample_job_dataset)} job descriptions\\\")\\n    print(f\\\"• Text processing rate: {len(sample_job_dataset)/total_time:.1f} jobs/second\\\")\\n    \\nexcept Exception as e:\\n    print(f\\\"\\\\n❌ Comprehensive analysis failed: {e}\\\")\\n    print(\\\"\\\\n🔍 Likely causes:\\\")\\n    print(\\\"  - API keys not configured correctly\\\")\\n    print(\\\"  - Missing dependencies\\\")\\n    print(\\\"  - Network connectivity issues\\\")\\n    print(\\\"  - API rate limits exceeded\\\")\\n\\n# Generate summary report\\nprint(f\\\"\\\\n\\\" + \\\"=\\\" * 80)\\nprint(\\\"📋 PROJECT SUMMARY\\\")\\nprint(\\\"=\\\" * 80)\\nprint(\\\"\\\"\\\"\\n🎯 JOB TREND ANALYZER - COMPLETE IMPLEMENTATION\\n\\n✅ IMPLEMENTED FEATURES:\\n• Text preprocessing and cleaning\\n• N-gram extraction with TF-IDF scoring\\n• Embedding generation using Together AI (m2-bert)\\n• K-means clustering for skill grouping\\n• LLM-powered analysis using Google Gemini\\n• LangChain tool integration\\n• End-to-end pipeline automation\\n\\n🔧 TECHNICAL COMPONENTS:\\n• Modular architecture with clear separation of concerns\\n• Error handling and fallback mechanisms\\n• Performance monitoring and metrics\\n• Scalable design for larger datasets\\n\\n📊 ANALYSIS CAPABILITIES:\\n• Skill trend identification\\n• Technology cluster discovery\\n• Market insight generation\\n• Career recommendation system\\n• Automated report generation\\n\\n🚀 NEXT STEPS:\\n1. Deploy as a web application using Streamlit\\n2. Add real-time job scraping capabilities\\n3. Implement time-series trend analysis\\n4. Create interactive visualizations\\n5. Build API endpoints for integration\\n6. Add more sophisticated NLP models\\n7. Implement caching for better performance\\n\\n💡 USAGE:\\n- For job seekers: Identify trending skills and career paths\\n- For employers: Understand market demands and skill gaps\\n- For educators: Align curriculum with industry needs\\n- For researchers: Analyze job market evolution\\n\\\"\\\"\\\")\\n\\nprint(f\\\"\\\\n🎉 Demo completed successfully!\\\")\\nprint(f\\\"📚 Check the generated reports and visualizations above.\\\")\\nprint(f\\\"🔗 All components are now ready for production use.\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
