# üìä Job Trend Analyzer

D·ª± √°n ph√¢n t√≠ch xu h∆∞·ªõng th·ªã tr∆∞·ªùng vi·ªác l√†m b·∫±ng c√°ch k·∫øt h·ª£p **n-gram + embedding + Gemini LLM Agent**, ƒë∆∞·ª£c tri·ªÉn khai chuy√™n nghi·ªáp theo ki·∫øn tr√∫c LangChain ho·∫∑c LangGraph.

---

## üß± Ki·∫øn tr√∫c t·ªïng quan

```mermaid
flowchart TD
  A[Job Descriptions] --> B[Text Cleaning]
  B --> C[N-gram Extractor]
  C --> D[Embedding with m2-bert]
  D --> E[Clustering]
  E --> F[LLM Agent (Gemini)]
  F --> G[Trend Report Output]
```

### üìÅ C·∫•u tr√∫c th∆∞ m·ª•c
```
job-trend-analyzer/
‚îÇ
‚îú‚îÄ‚îÄ data/                     # D·ªØ li·ªáu g·ªëc v√† ƒë√£ x·ª≠ l√Ω
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # D·ªØ li·ªáu raw (json, html, csv, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ processed/            # D·ªØ li·ªáu sau khi clean
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Th√¥ng tin config (API key, model, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py      # L√†m s·∫°ch vƒÉn b·∫£n
‚îÇ   ‚îú‚îÄ‚îÄ ngram_extractor.py    # T·∫°o n-gram
‚îÇ   ‚îú‚îÄ‚îÄ embedding.py          # G·ªçi Together API ƒë·ªÉ t·∫°o embedding
‚îÇ   ‚îú‚îÄ‚îÄ cluster.py            # Gom c·ª•m b·∫±ng k-means
‚îÇ   ‚îú‚îÄ‚îÄ llm_agent.py          # T∆∞∆°ng t√°c v·ªõi Gemini Agent
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py           # T√≠ch h·ª£p to√†n b·ªô pipeline
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                # Jupyter ph√¢n t√≠ch & tr·ª±c quan h√≥a
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ main.py
```

### ‚öôÔ∏è C√¥ng ngh·ªá s·ª≠ d·ª•ng
| Th√†nh ph·∫ßn   | C√¥ng ngh·ªá                                      |
|-------------|------------------------------------------------|
| LLM Agent   | Gemini API (google-generativeai)               |
| Embedding   | togethercomputer/m2-bert-80M-32k-retrieval      |
| Clustering  | scikit-learn (KMeans)                          |
| Orchestrator| LangChain ho·∫∑c LangGraph                       |
| Visualization| Streamlit, Gradio, Plotly (tu·ª≥ ch·ªçn)           |

---

## üîÑ Lu·ªìng x·ª≠ l√Ω chi ti·∫øt

1. **Clean vƒÉn b·∫£n**
```python
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    tokens = text.split()
    return " ".join([t for t in tokens if t not in STOPWORDS])
```

2. **T·∫°o n-gram**
```python
from sklearn.feature_extraction.text import CountVectorizer

def get_ngrams(texts, n=2, top_k=50):
    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')
    X = vectorizer.fit_transform(texts)
    freqs = zip(vectorizer.get_feature_names_out(), X.sum(axis=0).A1)
    return sorted(freqs, key=lambda x: x[1], reverse=True)[:top_k]
```

3. **Embedding v·ªõi Together API**
```python
from together import Together

def get_embeddings(phrases: list[str]):
    client = Together()
    result = []
    for phrase in phrases:
        response = client.embeddings.create(
            model="togethercomputer/m2-bert-80M-32k-retrieval",
            input=phrase
        )
        result.append((phrase, response.data[0].embedding))
    return result
```

4. **Gom c·ª•m (clustering)**
```python
from sklearn.cluster import KMeans

def cluster_embeddings(embeddings, n_clusters=10):
    vectors = [emb for _, emb in embeddings]
    phrases = [phrase for phrase, _ in embeddings]

    model = KMeans(n_clusters=n_clusters, random_state=42)
    labels = model.fit_predict(vectors)

    clusters = {i: [] for i in range(n_clusters)}
    for idx, label in enumerate(labels):
        clusters[label].append(phrases[idx])
    return clusters
```

5. **LLM Agent Gemini ph√¢n t√≠ch**
```python
from langchain_google_genai import ChatGoogleGenerativeAI

def analyze_clusters(clusters: dict):
    prompt = "D∆∞·ªõi ƒë√¢y l√† c√°c nh√≥m k·ªπ nƒÉng/ngh·ªÅ nghi·ªáp theo c·ª•m nghƒ©a:\n\n"
    for i, phrases in clusters.items():
        prompt += f"Nh√≥m {i+1}: {', '.join(phrases)}\n"
    prompt += "\nH√£y ph√¢n t√≠ch xu h∆∞·ªõng th·ªã tr∆∞·ªùng d·ª±a tr√™n c√°c nh√≥m tr√™n."

    model = ChatGoogleGenerativeAI(model="gemini-pro")
    return model.invoke(prompt)
```

6. **T√≠ch h·ª£p LangChain / LangGraph**

LangChain Tool:
```python
from langchain.tools import tool

@tool
def analyze_job_trend(texts: list[str]):
    cleaned = [clean_text(t) for t in texts]
    ngrams = get_ngrams(cleaned, n=2, top_k=100)
    phrases = [g[0] for g in ngrams]
    embeddings = get_embeddings(phrases)
    clusters = cluster_embeddings(embeddings)
    result = analyze_clusters(clusters)
    return result
```

LangGraph (s∆° ƒë·ªì pipeline d·∫°ng node)
```yaml
Node1: Clean ‚Üí Node2: N-gram ‚Üí Node3: Embed ‚Üí Node4: Cluster ‚Üí Node5: Analyze
```

---

---

## üöÄ C√°ch s·ª≠ d·ª•ng

### 1. C√†i ƒë·∫∑t dependencies
```bash
pip install -r requirements.txt
```

### 2. Thi·∫øt l·∫≠p API keys
T·∫°o file `.env` t·ª´ template:
```bash
cp .env.example .env
```

Ch·ªânh s·ª≠a file `.env` v·ªõi API keys c·ªßa b·∫°n:
```env
TOGETHER_API_KEY=your_together_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
```

### 3. Ch·∫°y ·ª©ng d·ª•ng

#### üåê Web UI (Khuy·∫øn ngh·ªã)
```bash
# C√°ch 1: S·ª≠ d·ª•ng batch file (Windows)
run_ui.bat

# C√°ch 2: S·ª≠ d·ª•ng Python launcher
python launch_ui.py

# C√°ch 3: Ch·∫°y tr·ª±c ti·∫øp Streamlit
streamlit run src\web_ui.py
```

#### üíª Command Line Interface
```bash
# Ch·∫ø ƒë·ªô demo v·ªõi d·ªØ li·ªáu m·∫´u
python main.py --demo

# Ph√¢n t√≠ch file c·ª• th·ªÉ
python main.py --input data/raw/sample_jobs.json --output results.json

# Xem t·∫•t c·∫£ options
python main.py --help
```

#### üìî Jupyter Notebook
```bash
jupyter notebook notebooks/job_trend_analyzer_demo.ipynb
```

## üìà V√≠ d·ª• k·∫øt qu·∫£ ƒë·∫ßu ra
```json
{
  "trend_summary": "C√°c nh√≥m k·ªπ nƒÉng v·ªÅ AI v√† backend ƒëang tƒÉng m·∫°nh, ƒë·∫∑c bi·ªát l√† 'machine learning', 'python backend' v√† 'data engineer'. Ng∆∞·ª£c l·∫°i, c√°c nh√≥m li√™n quan ƒë·∫øn legacy system nh∆∞ 'cobol developer' c√≥ xu h∆∞·ªõng gi·∫£m.",
  "clusters": {
    "AI/ML": ["machine learning", "deep learning", "ai engineer"],
    "Backend": ["java backend", "python backend", "spring boot"],
    ...
  }
}
```

---

## üöÄ M·ªü r·ªông t∆∞∆°ng lai
- G·∫Øn m·ªëc th·ªùi gian ‚Üí ph√¢n t√≠ch theo qu√Ω/nƒÉm
- L∆∞u k·∫øt qu·∫£ v√†o vector database (FAISS, Weaviate)
- Tr·ª±c quan h√≥a c·ª•m nghƒ©a b·∫±ng UMAP/PCA
- T·∫°o dashboard t∆∞∆°ng t√°c (Streamlit)

---

## üßæ Y√™u c·∫ßu h·ªá th·ªëng
| Th√†nh ph·∫ßn | M√¥ t·∫£ |
|------------|-------|
| Python     | >=3.10|
| Libraries  | together, scikit-learn, langchain, google-generativeai, sentence-transformers |
| API Keys   | Together API + Gemini API |

---

## ‚úÖ Ti·∫øp theo?
- Vi·∫øt module pipeline.py t√≠ch h·ª£p m·ªçi b∆∞·ªõc?
- T·∫°o l·ªánh CLI ho·∫∑c app Streamlit ch·∫°y t·ª´ d·ªØ li·ªáu th·ª±c?
- Cung c·∫•p t·∫≠p d·ªØ li·ªáu demo ƒë·ªÉ th·ª≠ nghi·ªám?
