"""
LLM Agent Module
S·ª≠ d·ª•ng Gemini API ƒë·ªÉ ph√¢n t√≠ch c√°c c·ª•m v√† t·∫°o b√°o c√°o xu h∆∞·ªõng
"""

import os
import time
import logging
from typing import List, Dict, Optional, Any, Union
from dataclasses import dataclass
import json
from enum import Enum

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AnalysisType(Enum):
    """Types of analysis the LLM agent can perform"""
    TREND_ANALYSIS = "trend_analysis"
    SKILL_GROUPING = "skill_grouping"
    MARKET_INSIGHTS = "market_insights"
    CAREER_RECOMMENDATIONS = "career_recommendations"

@dataclass
class AnalysisRequest:
    """Data class for analysis requests"""
    clusters: Dict[int, List[str]]
    analysis_type: AnalysisType
    context: Optional[str] = None
    max_tokens: int = 1000
    temperature: float = 0.3
    
    def to_dict(self) -> Dict:
        return {
            'clusters': self.clusters,
            'analysis_type': self.analysis_type.value,
            'context': self.context,
            'max_tokens': self.max_tokens,
            'temperature': self.temperature
        }

@dataclass
class AnalysisResult:
    """Data class for analysis results"""
    analysis_type: str
    summary: str
    detailed_analysis: Dict[str, Any]
    recommendations: List[str]
    trends: List[Dict[str, Any]]
    confidence_score: float
    timestamp: float
    
    def to_dict(self) -> Dict:
        return {
            'analysis_type': self.analysis_type,
            'summary': self.summary,
            'detailed_analysis': self.detailed_analysis,
            'recommendations': self.recommendations,
            'trends': self.trends,
            'confidence_score': self.confidence_score,
            'timestamp': self.timestamp
        }

class GeminiLLMAgent:
    """LLM Agent s·ª≠ d·ª•ng Google Gemini API"""
    
    def __init__(self,
                 api_key: Optional[str] = None,
                 model: str = "gemini-2.5-flash",
                 max_retries: int = 3,
                 retry_delay: float = 2.0):
        """
        Initialize Gemini LLM Agent
        
        Args:
            api_key: Google API key (if None, will try to get from environment)
            model: Model name to use
            max_retries: Maximum number of retries for failed requests
            retry_delay: Delay between retries in seconds
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        self.model = model
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        
        if not self.api_key:
            raise ValueError("Gemini API key is required. Set GEMINI_API_KEY environment variable.")
        
        # Initialize Gemini client
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            
            # Try to initialize with the specified model
            try:
                self.client = genai.GenerativeModel(model)
                logger.info(f"‚úÖ Gemini API client initialized with model: {self.model}")
            except Exception as model_error:
                # Fallback to other Gemini models if the specified model fails
                logger.warning(f"Failed to initialize {model}, trying fallback models...")
                fallback_models = ["gemini-2.5-flash", "gemini-1.5-flash", "gemini-1.5-pro", "gemini-pro"]
                
                for fallback_model in fallback_models:
                    try:
                        self.client = genai.GenerativeModel(fallback_model)
                        self.model = fallback_model
                        logger.info(f"‚úÖ Gemini API client initialized with fallback model: {self.model}")
                        break
                    except Exception as fallback_error:
                        logger.warning(f"Fallback model {fallback_model} also failed: {fallback_error}")
                        continue
                else:
                    raise Exception(f"All model attempts failed. Original error: {model_error}")
                    
        except ImportError:
            logger.error("google-generativeai library not installed. Please install with: pip install google-generativeai")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize Gemini client: {e}")
            raise
    
    def _build_prompt(self, request: AnalysisRequest) -> str:
        """Build prompt for the LLM based on analysis type"""
        
        # Base context
        prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m c√¥ng ngh·ªá. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c√°c nh√≥m k·ªπ nƒÉng/c√¥ng ngh·ªá v√† ƒë∆∞a ra nh·ªØng nh·∫≠n ƒë·ªãnh s√¢u s·∫Øc v·ªÅ xu h∆∞·ªõng th·ªã tr∆∞·ªùng.

D·ªØ li·ªáu ƒë·∫ßu v√†o - C√°c nh√≥m k·ªπ nƒÉng/c√¥ng ngh·ªá:
"""
        
        # Add cluster data
        for cluster_id, items in request.clusters.items():
            prompt += f"\nNh√≥m {cluster_id + 1}: {', '.join(items[:10])}"  # Limit items to avoid token overflow
            if len(items) > 10:
                prompt += f" (v√† {len(items) - 10} k·ªπ nƒÉng kh√°c)"
        
        # Add context if provided
        if request.context:
            prompt += f"\n\nB·ªëi c·∫£nh b·ªï sung: {request.context}"
        
        # Add specific instructions based on analysis type
        if request.analysis_type == AnalysisType.TREND_ANALYSIS:
            prompt += """

H√£y th·ª±c hi·ªán ph√¢n t√≠ch xu h∆∞·ªõng th·ªã tr∆∞·ªùng v·ªõi c√°c y√™u c·∫ßu sau:

1. **Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng**: X√°c ƒë·ªãnh nh√≥m k·ªπ nƒÉng n√†o ƒëang c√≥ xu h∆∞·ªõng tƒÉng m·∫°nh
2. **Xu h∆∞·ªõng suy gi·∫£m**: Nh√≥m k·ªπ nƒÉng n√†o c√≥ d·∫•u hi·ªáu suy gi·∫£m
3. **C√¥ng ngh·ªá m·ªõi n·ªïi**: Nh·ªØng c√¥ng ngh·ªá/k·ªπ nƒÉng m·ªõi ƒëang xu·∫•t hi·ªán
4. **K·ªπ nƒÉng c·ªët l√µi**: Nh·ªØng k·ªπ nƒÉng v·∫´n quan tr·ªçng v√† ·ªïn ƒë·ªãnh
5. **D·ª± ƒëo√°n t∆∞∆°ng lai**: Xu h∆∞·ªõng c√≥ th·ªÉ x·∫£y ra trong 1-2 nƒÉm t·ªõi

Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi format:
{
    "summary": "T√≥m t·∫Øt ng·∫Øn g·ªçn v·ªÅ xu h∆∞·ªõng th·ªã tr∆∞·ªùng",
    "trending_up": ["c√¥ng ngh·ªá tƒÉng tr∆∞·ªüng"],
    "trending_down": ["c√¥ng ngh·ªá suy gi·∫£m"],
    "emerging_tech": ["c√¥ng ngh·ªá m·ªõi n·ªïi"],
    "core_skills": ["k·ªπ nƒÉng c·ªët l√µi"],
    "future_predictions": ["d·ª± ƒëo√°n t∆∞∆°ng lai"],
    "confidence_score": 0.85
}"""
        
        elif request.analysis_type == AnalysisType.SKILL_GROUPING:
            prompt += """

H√£y ph√¢n t√≠ch v√† nh√≥m c√°c k·ªπ nƒÉng theo ti√™u ch√≠ sau:

1. Theo lƒ©nh v·ª±c: Frontend, Backend, Data Science, DevOps, AI/ML
2. Theo m·ª©c ƒë·ªô: Entry, Mid, Senior, Expert
3. Theo xu h∆∞·ªõng: Hot, Stable, Declining
4. Theo m·ªëi quan h·ªá: Complementary, Alternatives

Tr·∫£ v·ªÅ JSON v·ªõi format:
{
    "summary": "T√≥m t·∫Øt v·ªÅ c√°ch nh√≥m k·ªπ nƒÉng",
    "by_domain": {"Frontend": [], "Backend": [], "Data Science": [], "DevOps": [], "AI/ML": []},
    "by_level": {"Entry": [], "Mid": [], "Senior": [], "Expert": []},
    "by_trend": {"Hot": [], "Stable": [], "Declining": []},
    "skill_relationships": {"complementary": [], "alternatives": []},
    "confidence_score": 0.9
}"""
        
        elif request.analysis_type == AnalysisType.MARKET_INSIGHTS:
            prompt += """

H√£y ƒë∆∞a ra nh·ªØng insight s√¢u s·∫Øc v·ªÅ th·ªã tr∆∞·ªùng vi·ªác l√†m d·ª±a tr√™n d·ªØ li·ªáu:

1. **C∆° h·ªôi vi·ªác l√†m**: Lƒ©nh v·ª±c n√†o c√≥ nhi·ªÅu c∆° h·ªôi nh·∫•t
2. **M·ª©c l∆∞∆°ng**: Xu h∆∞·ªõng m·ª©c l∆∞∆°ng theo t·ª´ng nh√≥m k·ªπ nƒÉng
3. **Kho·∫£ng c√°ch k·ªπ nƒÉng**: K·ªπ nƒÉng n√†o b·ªã thi·∫øu h·ª•t tr√™n th·ªã tr∆∞·ªùng
4. **ƒê·ªãnh h∆∞·ªõng h·ªçc t·∫≠p**: N√™n h·ªçc k·ªπ nƒÉng g√¨ ƒë·ªÉ c√≥ c∆° h·ªôi vi·ªác l√†m t·ªët
5. **R·ªßi ro ngh·ªÅ nghi·ªáp**: Nh·ªØng r·ªßi ro c·∫ßn l∆∞u √Ω

Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi format:
{
    "summary": "T√≥m t·∫Øt insights v·ªÅ th·ªã tr∆∞·ªùng",
    "job_opportunities": {"high": [], "medium": [], "low": []},
    "salary_trends": {"high_paying": [], "average_paying": [], "entry_level": []},
    "skill_gaps": ["k·ªπ nƒÉng b·ªã thi·∫øu h·ª•t"],
    "learning_recommendations": ["k·ªπ nƒÉng n√™n h·ªçc"],
    "career_risks": ["r·ªßi ro ngh·ªÅ nghi·ªáp"],
    "confidence_score": 0.8
}"""
        
        elif request.analysis_type == AnalysisType.CAREER_RECOMMENDATIONS:
            prompt += """

H√£y ƒë∆∞a ra l·ªùi khuy√™n ngh·ªÅ nghi·ªáp d·ª±a tr√™n ph√¢n t√≠ch c√°c nh√≥m k·ªπ nƒÉng:

1. **L·ªô tr√¨nh ngh·ªÅ nghi·ªáp**: C√°c con ƒë∆∞·ªùng ph√°t tri·ªÉn kh√°c nhau
2. **K·ªπ nƒÉng c·∫ßn thi·∫øt**: K·ªπ nƒÉng c·∫ßn c√≥ cho t·ª´ng l·ªô tr√¨nh
3. **Th·ªùi gian h·ªçc t·∫≠p**: ∆Ø·ªõc t√≠nh th·ªùi gian ƒë·ªÉ th√†nh th·∫°o
4. **L·ªùi khuy√™n chuy·ªÉn ngh·ªÅ**: Cho ng∆∞·ªùi mu·ªën chuy·ªÉn sang IT
5. **C·∫≠p nh·∫≠t k·ªπ nƒÉng**: Cho ng∆∞·ªùi ƒë√£ l√†m trong ng√†nh

Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON v·ªõi format:
{
    "summary": "T√≥m t·∫Øt l·ªùi khuy√™n ngh·ªÅ nghi·ªáp",
    "career_paths": [{"path": "t√™n l·ªô tr√¨nh", "skills": [], "duration": "th·ªùi gian"}],
    "essential_skills": ["k·ªπ nƒÉng c·∫ßn thi·∫øt"],
    "learning_timeline": {"beginner": "6-12 th√°ng", "intermediate": "1-2 nƒÉm"},
    "career_switch_advice": ["l·ªùi khuy√™n chuy·ªÉn ngh·ªÅ"],
    "skill_update_advice": ["l·ªùi khuy√™n c·∫≠p nh·∫≠t k·ªπ nƒÉng"],
    "confidence_score": 0.85
}"""
        
        prompt += "\n\nL∆∞u √Ω: Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá, kh√¥ng c√≥ text b·ªï sung."
        
        return prompt
    
    def _make_prompt_neutral(self, prompt: str) -> str:
        """Make prompt more neutral to avoid safety blocks"""
        # Remove potentially triggering words and make more neutral
        neutral_prompt = prompt.replace("xu h∆∞·ªõng th·ªã tr∆∞·ªùng", "ph√¢n t√≠ch d·ªØ li·ªáu")
        neutral_prompt = neutral_prompt.replace("vi·ªác l√†m", "th√¥ng tin")
        neutral_prompt = neutral_prompt.replace("tƒÉng tr∆∞·ªüng", "ph√°t tri·ªÉn")
        neutral_prompt = neutral_prompt.replace("suy gi·∫£m", "thay ƒë·ªïi")
        
        # Make the request more academic/technical
        neutral_prompt = "B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu. " + neutral_prompt
        
        return neutral_prompt
    
    def _create_fallback_response(self) -> str:
        """Create a fallback JSON response when LLM fails"""
        fallback = {
            "summary": "Ph√¢n t√≠ch kh√¥ng th·ªÉ ho√†n th√†nh do gi·ªõi h·∫°n API. Vui l√≤ng th·ª≠ l·∫°i sau.",
            "trending_up": ["python", "javascript", "react", "aws"],
            "trending_down": [],
            "emerging_tech": ["ai", "machine learning"],
            "core_skills": ["programming", "database", "api"],
            "confidence_score": 0.1
        }
        return json.dumps(fallback, ensure_ascii=False, indent=2)
    
    def _call_llm_with_retry(self, prompt: str, **kwargs) -> str:
        """Call LLM with retry logic and safety handling"""
        
        # Log the prompt being sent
        logger.info("üì§ SENDING PROMPT TO GEMINI:")
        logger.info("=" * 80)
        logger.info(prompt[:1000] + "..." if len(prompt) > 1000 else prompt)
        logger.info("=" * 80)
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"üîÑ Attempt {attempt + 1}/{self.max_retries} with model: {self.model}")
                
                # Start with simple generation without safety settings
                if attempt == 0:
                    response = self.client.generate_content(prompt)
                else:
                    # If first attempt failed, try with generation config
                    response = self.client.generate_content(
                        prompt,
                        generation_config={
                            'temperature': kwargs.get('temperature', 0.3),
                            'max_output_tokens': kwargs.get('max_tokens', 2000),
                        }
                    )
                
                logger.info(f"üì• Response received. Type: {type(response)}")
                
                # Check if we have a valid response
                if hasattr(response, 'text') and response.text:
                    logger.info(f"‚úÖ Valid response received ({len(response.text)} chars)")
                    logger.info("üì• RESPONSE FROM GEMINI:")
                    logger.info("=" * 80)
                    logger.info(response.text[:500] + "..." if len(response.text) > 500 else response.text)
                    logger.info("=" * 80)
                    return response.text.strip()
                
                # Handle blocked responses (finish_reason = 2 means SAFETY)
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    finish_reason = getattr(candidate, 'finish_reason', None)
                    
                    logger.warning(f"‚ùå Response candidate finish_reason: {finish_reason}")
                    
                    if finish_reason == 2:  # SAFETY
                        logger.warning(f"üö´ Response blocked by safety filter on attempt {attempt + 1}")
                        # Try with a more neutral prompt
                        if attempt == 0:
                            neutral_prompt = self._make_prompt_neutral(prompt)
                            logger.info("üîÑ Trying with neutralized prompt...")
                            prompt = neutral_prompt
                            continue
                    elif finish_reason == 3:  # RECITATION
                        logger.warning(f"üö´ Response blocked for recitation on attempt {attempt + 1}")
                    else:
                        logger.warning(f"üö´ Response blocked with finish_reason {finish_reason} on attempt {attempt + 1}")
                
                logger.warning(f"‚ö†Ô∏è Empty or blocked response on attempt {attempt + 1}")
                    
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (2 ** attempt))
                else:
                    logger.error(f"All attempts failed: {e}")
                    # Return a fallback response instead of raising
                    return self._create_fallback_response()
        
        # If all retries failed, return fallback
        return self._create_fallback_response()
    
    def _make_prompt_neutral(self, prompt: str) -> str:
        """Make prompt more neutral to avoid safety blocks"""
        # Create a much simpler, more academic prompt
        simple_prompt = """Ph√¢n t√≠ch d·ªØ li·ªáu k·ªπ nƒÉng c√¥ng ngh·ªá sau ƒë√¢y v√† nh√≥m ch√∫ng theo c√°c ti√™u ch√≠:

D·ªØ li·ªáu: """
        
        # Add cluster data in a simple format
        for cluster_id, items in self.current_clusters.items():
            simple_prompt += f"\nNh√≥m {cluster_id + 1}: {', '.join(items[:5])}"
        
        simple_prompt += """

H√£y tr·∫£ v·ªÅ JSON v·ªõi format:
{
    "summary": "T√≥m t·∫Øt",
    "by_domain": {"Frontend": [], "Backend": [], "AI": []},
    "confidence_score": 0.8
}"""
        
        return simple_prompt
    
    def _create_fallback_response(self) -> str:
        """Create a fallback response when LLM fails"""
        fallback = {
            "summary": "Ph√¢n t√≠ch t·ª± ƒë·ªông kh√¥ng kh·∫£ d·ª•ng. D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng nh∆∞ng kh√¥ng th·ªÉ t·∫°o b√°o c√°o chi ti·∫øt.",
            "trending_up": ["python", "machine learning", "cloud"],
            "trending_down": [],
            "emerging_tech": ["ai", "data science"],
            "core_skills": ["programming", "software development"],
            "future_predictions": ["Nhu c·∫ßu v·ªÅ k·ªπ nƒÉng c√¥ng ngh·ªá ti·∫øp t·ª•c tƒÉng"],
            "confidence_score": 0.3
        }
        return json.dumps(fallback, ensure_ascii=False)
    
    def analyze(self, request: AnalysisRequest) -> AnalysisResult:
        """
        Perform analysis using the LLM
        
        Args:
            request: AnalysisRequest object
            
        Returns:
            AnalysisResult object
        """
        logger.info(f"ü§ñ Starting {request.analysis_type.value} analysis...")
        
        # Store clusters for neutral prompt fallback
        self.current_clusters = request.clusters
        
        # Build prompt
        prompt = self._build_prompt(request)
        
        # Call LLM
        response_text = self._call_llm_with_retry(
            prompt,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        # Parse JSON response
        try:
            # Try to extract JSON from response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                parsed_response = json.loads(json_text)
            else:
                # Fallback: treat entire response as summary
                parsed_response = {"summary": response_text, "confidence_score": 0.5}
                
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON response: {e}")
            parsed_response = {"summary": response_text, "confidence_score": 0.3}
        
        # Extract information from parsed response
        summary = parsed_response.get("summary", "No summary available")
        confidence_score = parsed_response.get("confidence_score", 0.5)
        
        # Extract detailed analysis (everything except summary and confidence)
        detailed_analysis = {k: v for k, v in parsed_response.items() 
                           if k not in ["summary", "confidence_score", "recommendations"]}
        
        # Extract recommendations
        recommendations = parsed_response.get("recommendations", [])
        
        # Extract trends
        trends = []
        if "trending_up" in parsed_response:
            trends.extend([{"type": "growing", "items": parsed_response["trending_up"]}])
        if "trending_down" in parsed_response:
            trends.extend([{"type": "declining", "items": parsed_response["trending_down"]}])
        if "emerging_tech" in parsed_response:
            trends.extend([{"type": "emerging", "items": parsed_response["emerging_tech"]}])
        
        result = AnalysisResult(
            analysis_type=request.analysis_type.value,
            summary=summary,
            detailed_analysis=detailed_analysis,
            recommendations=recommendations,
            trends=trends,
            confidence_score=confidence_score,
            timestamp=time.time()
        )
        
        logger.info(f"‚úÖ Analysis completed with confidence score: {confidence_score:.2f}")
        return result

# Convenience functions
def analyze_clusters(clusters: Dict[int, List[str]], 
                    analysis_type: str = "trend_analysis",
                    api_key: Optional[str] = None,
                    context: Optional[str] = None) -> AnalysisResult:
    """
    Quick function to analyze clusters
    
    Args:
        clusters: Dictionary mapping cluster IDs to lists of items
        analysis_type: Type of analysis to perform
        api_key: Gemini API key
        context: Additional context for analysis
        
    Returns:
        AnalysisResult object
    """
    agent = GeminiLLMAgent(api_key=api_key)
    
    request = AnalysisRequest(
        clusters=clusters,
        analysis_type=AnalysisType(analysis_type),
        context=context
    )
    
    return agent.analyze(request)

def analyze_job_trends(cluster_results: List[Any],
                      api_key: Optional[str] = None) -> AnalysisResult:
    """
    Analyze job trends from cluster results
    
    Args:
        cluster_results: List of ClusterResult objects
        api_key: Gemini API key
        
    Returns:
        AnalysisResult object
    """
    # Convert cluster results to dictionary format
    clusters = {}
    for i, cluster in enumerate(cluster_results):
        clusters[i] = cluster.items if hasattr(cluster, 'items') else cluster
    
    return analyze_clusters(clusters, "trend_analysis", api_key)

# Example usage and testing
if __name__ == "__main__":
    # Test data
    sample_clusters = {
        0: ["python developer", "machine learning engineer", "data scientist"],
        1: ["javascript developer", "react developer", "frontend engineer"],
        2: ["java backend", "spring boot", "microservices"],
        3: ["cloud engineer", "aws developer", "devops engineer"],
        4: ["mobile developer", "react native", "flutter developer"]
    }
    
    print("üß™ Testing LLM Agent")
    print("=" * 50)
    
    # Check if API key is available
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("‚ö†Ô∏è  GEMINI_API_KEY not found in environment variables")
        print("Set your API key to test the LLM functionality")
        print("Example: set GEMINI_API_KEY=your_api_key_here")
    else:
        try:
            # Test available models first
            print("üîç Testing available Gemini models...")
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            
            models = genai.list_models()
            available_models = []
            for model in models:
                if 'generateContent' in model.supported_generation_methods:
                    available_models.append(model.name)
                    print(f"   ‚úÖ {model.name}")
            
            if not available_models:
                print("   ‚ùå No compatible models found")
                exit(1)
            
            # Test trend analysis with first available model
            model_name = available_models[0].replace('models/', '')
            print(f"\nü§ñ Testing with model: {model_name}")
            
            agent = GeminiLLMAgent(model=model_name)
            
            request = AnalysisRequest(
                clusters=sample_clusters,
                analysis_type=AnalysisType.TREND_ANALYSIS,
                context="D·ªØ li·ªáu t·ª´ c√°c job posting trong Q4 2024"
            )
            
            result = agent.analyze(request)
            
            print(f"‚úÖ Analysis completed:")
            print(f"   Type: {result.analysis_type}")
            print(f"   Confidence: {result.confidence_score:.2f}")
            print(f"   Summary: {result.summary[:200]}...")
            print(f"   Trends found: {len(result.trends)}")
            print(f"   Recommendations: {len(result.recommendations)}")
            
        except Exception as e:
            print(f"‚ùå Error during testing: {e}")
            print("Make sure your Gemini API key is valid and you have internet connection")
